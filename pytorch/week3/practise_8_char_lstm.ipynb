{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 诗经"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "诗经·国风·周南·关雎\n",
    "关关雎鸠，在河之洲。窈窕淑女，君子好逑。\n",
    "参差荇菜，左右流之。窈窕淑女，寤寐求之。\n",
    "求之不得，寤寐思服。悠哉悠哉，辗转反侧。\n",
    "参差荇菜，左右采之。窈窕淑女，琴瑟友之。\n",
    "参差荇菜，左右芼之。窈窕淑女，钟鼓乐之。\n",
    "\n",
    "诗经·国风·周南·葛覃\n",
    "葛之覃兮，施于中谷，维叶萋萋。黄鸟于飞，集于灌木，其鸣喈喈。\n",
    "葛之覃兮，施于中谷，维叶莫莫。是刈是濩，为绖为粥，效禄无疆。\n",
    "葛之覃兮，施于中谷，维叶萋萋。黄鸟于飞，集于灌木，其鸣喈喈。\n",
    "\n",
    "诗经·国风·周南·卷耳\n",
    "采采卷耳，不盈顷筐。嗟我怀人，置彼周行。\n",
    "陟彼崔嵬，我马虺虺。我姑酌彼金罍，维以不永怀。\n",
    "陟彼高冈，我马玄黄。我姑酌彼兕觥，维以不永伤。\n",
    "陟彼砠矣，我马瘏矣。我仆痡矣，云何吁矣。\n",
    "\n",
    "诗经·国风·周南·樛木\n",
    "南有樛木，葛藟累之。乐只君子，福履绥之。\n",
    "南有樛木，葛藟荒之。乐只君子，福履将之。\n",
    "南有樛木，葛藟萦之。乐只君子，福履成之。\n",
    "\n",
    "诗经·国风·周南·螽斯\n",
    "螽斯羽，诜诜兮。宜尔子孙，振振兮。\n",
    "螽斯羽，薨薨兮。宜尔子孙，绳绳兮。\n",
    "螽斯羽，揖揖兮。宜尔子孙，蛰蛰兮。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练，词汇表大小: 143，序列长度: 20\n",
      "Epoch 20/200, Loss: 4.3526\n",
      "Sample text:\n",
      "关雎灌差子求之，之·何之是经，。周。。·。，螽风于，，之，，。葛，窈嵬，履木·，风南，，，。采于卷，木·\n",
      "\n",
      "Epoch 40/200, Loss: 2.8431\n",
      "Sample text:\n",
      "关雎以寤陟不永，效。陟，嵬姑，。酌侧效彼，，彼我崔我不疆，我。酌我，酌，，为绖我我永，葛虺。我我痡，陟以\n",
      "\n",
      "Epoch 60/200, Loss: 1.3654\n",
      "Sample text:\n",
      "关雎关木，维维，之。。诗差淑，。左右采之。，只君，子福履，之。诗经国国，风····葛，葛之。兮，于，中，\n",
      "\n",
      "Epoch 80/200, Loss: 0.6595\n",
      "Sample text:\n",
      "关雎飞，关中灌，，莫喈莫。黄，。于，集灌灌，莫喈喈喈。诗，·。·周周·，卷卷耳，耳不，顷。。嗟我怀，置彼\n",
      "\n",
      "Epoch 100/200, Loss: 0.3881\n",
      "Sample text:\n",
      "关雎风·周周南，螽斯斯诜，宜，。。。我尔哉，，何思之。参，荇。，左右流，。窈窕悠女，，瑟友。。参差，菜，\n",
      "\n",
      "Epoch 120/200, Loss: 0.2588\n",
      "Sample text:\n",
      "关雎木，葛藟荒，。乐只君子，，。成之。诗经，国风·周南·，卷。采采不不，，顷顷，。嗟，彼人。置周矣，陟彼\n",
      "\n",
      "Epoch 140/200, Loss: 0.2064\n",
      "Sample text:\n",
      "关雎风·周南·，雎葛采覃覃兮，。君中谷，维，莫莫。是刈是，，。为为。。，高矣，我马瘏，。我。痡矣，，何吁\n",
      "\n",
      "Epoch 160/200, Loss: 0.1503\n",
      "Sample text:\n",
      "关雎风·周南·，螽螽斯羽，诜，。洲宜我不，，云何吁。。诗，·。·周南·，耳采采卷耳，，盈顷。。嗟我，人，\n",
      "\n",
      "Epoch 180/200, Loss: 0.1166\n",
      "Sample text:\n",
      "关雎木，葛藟萦，。乐只君子，，。成之。诗经，国风·周南·，耳。采卷耳，，盈顷筐。嗟我，人，。彼玄黄，我彼\n",
      "\n",
      "Epoch 200/200, Loss: 0.0947\n",
      "Sample text:\n",
      "关雎矣，我马虺，。我姑酌彼金，。我不永怀。，彼高冈，我马，黄。我姑酌彼，觥维维以永伤，陟彼。矣，我，瘏矣\n",
      "\n",
      "\n",
      "训练完成！\n",
      "\n",
      "可用的起始短语示例（前10个）：\n",
      "·关, ·卷, ·周, ·国, ·樛, ·葛, ·螽, 。乐, 。南, 。参\n",
      "\n",
      "生成示例：\n",
      "\n",
      "起始：\"南有\"\n",
      "生成：南有葛葛荒之。，只君子，福成，。。南有樛木，葛藟荒之。乐，君。，福履将，。南有樛木，，藟萦。。乐只，子，福履将之，南有樛。，葛，萦之。乐只君，，福履成。。，经·国风·周，卷螽斯采不。，，彼金罍，我，不永怀\n",
      "\n",
      "起始：\"采采\"\n",
      "生成：采采之。窈窕淑，，琴瑟友之。，。荇菜，左右，之。窈窕淑女，钟。乐之。诗，·国风·周南，卷耳。采卷耳，，盈顷筐。嗟，怀人，。彼周，。陟彼崔，，，马虺虺。。姑，彼金罍，维以，永怀。陟彼。，，我马玄黄。，姑酌彼\n",
      "\n",
      "起始：\"君子\"\n",
      "生成：君子，福履绥之，南有樛木，葛，。之。乐只君，，履履绥之。，有。木，葛藟，之。乐只君子，福履。之。南，樛木，葛藟荒，。乐只。子，，履将之。南有，木，藟鸣。之，乐只子，福履，之。诗经·。，南葛采覃之。，淑女，\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers, dropout=0.2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 添加dropout来防止过拟合\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, n_layers, \n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # LSTM需要两个隐藏状态\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "\n",
    "def process_data(text):\n",
    "    # 去除空白字符\n",
    "    text = ''.join(text.split())\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char\n",
    "\n",
    "def create_sequences(text, char_to_idx, seq_length):\n",
    "    text = ''.join(text.split())\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(0, len(text) - seq_length, 3):  # 步长为3，增加数据多样性\n",
    "        sequence = text[i:i+seq_length]\n",
    "        target = text[i+1:i+seq_length+1]\n",
    "        \n",
    "        if len(sequence) == seq_length and len(target) == seq_length:\n",
    "            x.append([char_to_idx[char] for char in sequence])\n",
    "            y.append([char_to_idx[char] for char in target])\n",
    "    \n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    return x, y\n",
    "\n",
    "def train_model(model, data, targets, criterion, optimizer, vocab_size, batch_size=64):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    n_batches = len(data) // batch_size\n",
    "    if n_batches == 0:\n",
    "        n_batches = 1\n",
    "        batch_size = len(data)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(data))\n",
    "        batch_data = data[start:end]\n",
    "        batch_targets = targets[start:end]\n",
    "        \n",
    "        current_batch_size = len(batch_data)\n",
    "        \n",
    "        x = torch.zeros(current_batch_size, len(batch_data[0]), vocab_size)\n",
    "        for i, sequence in enumerate(batch_data):\n",
    "            for t, char_idx in enumerate(sequence):\n",
    "                x[i, t, char_idx] = 1\n",
    "                \n",
    "        hidden = model.init_hidden(current_batch_size)\n",
    "        \n",
    "        # 分离隐藏状态，防止梯度累积\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "            \n",
    "        output, hidden = model(x, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), batch_targets.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def generate_text(model, initial_str, char_to_idx, idx_to_char, vocab_size, \n",
    "                 pred_length=100, temperature=0.8):\n",
    "    # 确保初始字符串中的所有字符都在词汇表中\n",
    "    for char in initial_str:\n",
    "        if char not in char_to_idx:\n",
    "            raise ValueError(f\"Character '{char}' not in vocabulary\")\n",
    "    model.eval()\n",
    "    current_str = initial_str\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(pred_length):\n",
    "            x = torch.zeros(1, 1, vocab_size)\n",
    "            x[0, 0, char_to_idx[current_str[-1]]] = 1\n",
    "            \n",
    "            output, hidden = model(x, hidden)\n",
    "            \n",
    "            # 使用temperature参数来控制生成的随机性\n",
    "            output_dist = output[0, -1].div(temperature).exp()\n",
    "            probs = output_dist.div(torch.sum(output_dist))\n",
    "            \n",
    "            # 按概率采样并确保索引在有效范围内\n",
    "            char_idx = torch.multinomial(probs, 1)[0].item()\n",
    "            if char_idx >= len(idx_to_char):\n",
    "                char_idx = char_idx % len(idx_to_char)\n",
    "            \n",
    "            # 添加标点符号的逻辑\n",
    "            if len(current_str) % 7 == 0:  # 每7个字符加入逗号\n",
    "                current_str += '，'\n",
    "            elif len(current_str) % 15 == 0:  # 每15个字符加入句号\n",
    "                current_str += '。'\n",
    "            else:\n",
    "                current_str += idx_to_char[char_idx]\n",
    "            \n",
    "    return current_str\n",
    "\n",
    "def find_valid_starts(text, length=2):\n",
    "    \"\"\"找出文本中所有可用的起始短语\"\"\"\n",
    "    text = ''.join(text.split())\n",
    "    valid_starts = set()\n",
    "    for i in range(len(text) - length + 1):\n",
    "        valid_starts.add(text[i:i+length])\n",
    "    return sorted(list(valid_starts))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 增加序列长度和隐藏层大小\n",
    "    seq_length = 20\n",
    "    hidden_size = 256\n",
    "    n_layers = 2\n",
    "    \n",
    "    chars, char_to_idx, idx_to_char = process_data(text)\n",
    "    x, y = create_sequences(text, char_to_idx, seq_length)\n",
    "    \n",
    "    input_size = len(chars)\n",
    "    vocab_size = len(chars)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = CharRNN(input_size, hidden_size, vocab_size, n_layers, dropout=0.2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "    \n",
    "    # 增加训练轮数\n",
    "    n_epochs = 200\n",
    "    \n",
    "    print(f\"开始训练，词汇表大小: {vocab_size}，序列长度: {seq_length}\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        loss = train_model(model, x, y, criterion, optimizer, vocab_size)\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss:.4f}\")\n",
    "            # 每20轮展示一次生成效果\n",
    "            sample = generate_text(model, \"关雎\", char_to_idx, idx_to_char, \n",
    "                                 vocab_size, pred_length=50)\n",
    "            print(f\"Sample text:\\n{sample}\\n\")\n",
    "    \n",
    "    print(\"\\n训练完成！\")\n",
    "    \n",
    "    # 找出可用的起始文本\n",
    "    valid_starts = find_valid_starts(text)\n",
    "    print(\"\\n可用的起始短语示例（前10个）：\")\n",
    "    print(\", \".join(valid_starts[:10]))\n",
    "    \n",
    "    print(\"\\n生成示例：\")\n",
    "    \n",
    "    # 从训练数据中选择起始文本\n",
    "    initial_texts = [\"南有\", \"采采\", \"君子\"]\n",
    "    for init_text in initial_texts:\n",
    "        generated = generate_text(model, init_text, char_to_idx, idx_to_char, \n",
    "                                vocab_size, pred_length=100)\n",
    "        print(f\"\\n起始：\\\"{init_text}\\\"\\n生成：{generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
