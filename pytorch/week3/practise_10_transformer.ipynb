{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始分数: tensor([[8., 4.]])\n",
      "缩放后的分数: tensor([[5.6569, 2.8284]])\n",
      "注意力权重: tensor([[0.9442, 0.0558]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个简单的例子\n",
    "def simple_attention_example():\n",
    "    # 创建查询和键向量\n",
    "    query = torch.tensor([[2.0, 3.0]])  # 1x2\n",
    "    key = torch.tensor([[1.0, 2.0],\n",
    "                       [0.5, 1.0]])      # 2x2\n",
    "    \n",
    "    # 计算注意力分数\n",
    "    scores = torch.matmul(query, key.T)  # 1x2\n",
    "    print(\"原始分数:\", scores)\n",
    "    \n",
    "    # 缩放\n",
    "    d_k = query.size(-1)  # = 2\n",
    "    scaled_scores = scores / torch.sqrt(torch.tensor(d_k).float())\n",
    "    print(\"缩放后的分数:\", scaled_scores)\n",
    "    \n",
    "    # 应用softmax\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_scores, dim=-1)\n",
    "    print(\"注意力权重:\", attention_weights)\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "# 运行示例\n",
    "attention_weights = simple_attention_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False],\n",
      "         [ True,  True, False, False],\n",
      "         [ True,  True,  True, False],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "nopeak_mask = (1 - torch.triu(torch.ones(1, 4, 4), diagonal=1)).bool()\n",
    "\n",
    "print(nopeak_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: (2, 4, 6)\n",
      "输出形状: (2, 4, 2, 2, 6)\n",
      "注意力权重形状: (2, 4, 2, 4)\n",
      "\n",
      "第一个样本的注意力权重矩阵:\n",
      "[[[0.06442246 0.05851506 0.012118   0.86494448]\n",
      "  [0.07135287 0.0096341  0.59548844 0.32352459]]\n",
      "\n",
      " [[0.04042779 0.322952   0.49723829 0.13938192]\n",
      "  [0.14238434 0.72594244 0.0502511  0.08142212]]\n",
      "\n",
      " [[0.00665577 0.22435428 0.49451061 0.27447934]\n",
      "  [0.17749634 0.36344675 0.08819053 0.37086638]]\n",
      "\n",
      " [[0.50402078 0.18061427 0.26243141 0.05293353]\n",
      "  [0.22724647 0.55528546 0.12544975 0.09201832]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleAttention:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        # 初始化权重矩阵\n",
    "        self.Wq = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)  # Query权重\n",
    "        self.Wk = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)  # Key权重\n",
    "        self.Wv = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)  # Value权重\n",
    "        self.Wo = np.random.randn(hidden_dim, input_dim) / np.sqrt(hidden_dim)  # 输出权重\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        # 实现softmax函数\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # 减去最大值防止数值溢出\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: 输入序列，shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        # 1. 生成Query、Key、Value\n",
    "        Q = np.dot(X, self.Wq)  # (batch_size, seq_len, hidden_dim)\n",
    "        K = np.dot(X, self.Wk)  # (batch_size, seq_len, hidden_dim)\n",
    "        V = np.dot(X, self.Wv)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 2. 计算注意力分数\n",
    "        scores = np.dot(Q, K.transpose(0, 2, 1))  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # 3. 缩放注意力分数\n",
    "        scores = scores / np.sqrt(K.shape[-1])\n",
    "        \n",
    "        # 4. 应用softmax得到注意力权重\n",
    "        attention_weights = self.softmax(scores)  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # 5. 加权求和得到上下文向量\n",
    "        context = np.dot(attention_weights, V)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 6. 线性变换得到输出\n",
    "        output = np.dot(context, self.Wo)  # (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试代码\n",
    "def test_attention():\n",
    "    # 设置参数\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    input_dim = 6\n",
    "    hidden_dim = 8\n",
    "    \n",
    "    # 创建输入数据\n",
    "    X = np.random.randn(batch_size, seq_len, input_dim)\n",
    "    \n",
    "    # 初始化注意力层\n",
    "    attention = SimpleAttention(input_dim, hidden_dim)\n",
    "    \n",
    "    # 前向传播\n",
    "    output, attention_weights = attention.forward(X)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(\"输入形状:\", X.shape)\n",
    "    print(\"输出形状:\", output.shape)\n",
    "    print(\"注意力权重形状:\", attention_weights.shape)\n",
    "    print(\"\\n第一个样本的注意力权重矩阵:\")\n",
    "    print(attention_weights[0])\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 运行测试\n",
    "if __name__ == \"__main__\":\n",
    "    output, weights = test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total trainable parameters: 1393164\n",
      "Epoch: 0, Batch: 0, Loss: 2.6125\n",
      "Epoch: 0, Average Loss: 2.0882\n",
      "Epoch: 1, Batch: 0, Loss: 1.8289\n",
      "Epoch: 1, Average Loss: 1.6931\n",
      "Epoch: 2, Batch: 0, Loss: 1.5439\n",
      "Epoch: 2, Average Loss: 1.5102\n",
      "Epoch: 3, Batch: 0, Loss: 1.4064\n",
      "Epoch: 3, Average Loss: 1.4020\n",
      "Epoch: 4, Batch: 0, Loss: 1.3777\n",
      "Epoch: 4, Average Loss: 1.3074\n",
      "Epoch: 5, Batch: 0, Loss: 1.3067\n",
      "Epoch: 5, Average Loss: 1.1816\n",
      "Epoch: 6, Batch: 0, Loss: 1.1069\n",
      "Epoch: 6, Average Loss: 1.0490\n",
      "Epoch: 7, Batch: 0, Loss: 0.9880\n",
      "Epoch: 7, Average Loss: 0.9109\n",
      "Epoch: 8, Batch: 0, Loss: 0.8771\n",
      "Epoch: 8, Average Loss: 0.8041\n",
      "Epoch: 9, Batch: 0, Loss: 0.6553\n",
      "Epoch: 9, Average Loss: 0.7156\n",
      "Input sequence: [3, 1, 4, 1, 5]\n",
      "Predicted sequence: [1 5 1 4 1 3 2]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_masks(src, tgt):\n",
    "    # 创建源序列掩码\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    if tgt is None:  # 在推理时tgt可能为None\n",
    "        return src_mask, None\n",
    "    \n",
    "    # 创建目标序列掩码\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "    seq_length = tgt.size(1)\n",
    "    \n",
    "    # 创建自回归掩码\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones((1, seq_length, seq_length)), diagonal=1)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask.to(tgt.device)\n",
    "    \n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        output = self.W_o(output)\n",
    "        return output, attention_weights\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        attn1_output, _ = self.mha1(x, x, x, tgt_mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn1_output))\n",
    "        \n",
    "        attn2_output, _ = self.mha2(x, enc_output, enc_output, src_mask)\n",
    "        x = self.layernorm2(x + self.dropout(attn2_output))\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorm3(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def encode(self, src, src_mask):\n",
    "        x = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, src_mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, enc_output, src_mask, tgt_mask):\n",
    "        x = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        \n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, enc_output, src_mask, tgt_mask)\n",
    "            \n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        enc_output = self.encode(src, src_mask)\n",
    "        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "        return dec_output\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, max_len=10):\n",
    "        self.data = []\n",
    "        for _ in range(num_samples):\n",
    "            length = np.random.randint(3, max_len)\n",
    "            src = np.random.randint(1, 10, size=length)\n",
    "            tgt = np.flip(src).copy()\n",
    "            # 添加开始符号（1）和结束符号（2）\n",
    "            tgt = np.concatenate([[1], tgt, [2]])\n",
    "            src = torch.LongTensor(src)\n",
    "            tgt = torch.LongTensor(tgt)\n",
    "            self.data.append((src, tgt))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            src_mask, tgt_mask = create_masks(src, tgt_input)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            \n",
    "            loss = criterion(output.contiguous().view(-1, output.size(-1)), \n",
    "                           tgt_output.contiguous().view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            # 梯度裁剪，防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "def predict(model, src, device, max_len=50):\n",
    "    model.eval()\n",
    "    src = torch.LongTensor(src).unsqueeze(0).to(device)\n",
    "    src_mask, _ = create_masks(src, None)\n",
    "    \n",
    "    enc_output = model.encode(src, src_mask)\n",
    "    \n",
    "    # 开始符号\n",
    "    output = torch.LongTensor([[1]]).to(device)  # 1 是开始符号\n",
    "    \n",
    "    for _ in range(max_len-1):\n",
    "        _, tgt_mask = create_masks(src, output)\n",
    "        \n",
    "        dec_output = model.decode(output, enc_output, src_mask, tgt_mask)\n",
    "        prob = torch.softmax(dec_output[:, -1], dim=-1)\n",
    "        pred = prob.argmax(dim=-1)\n",
    "        \n",
    "        output = torch.cat([output, pred.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        if pred.item() == 2:  # 2 是结束符号\n",
    "            break\n",
    "    \n",
    "    return output.squeeze().cpu().numpy()\n",
    "\n",
    "def main():\n",
    "    # 设置随机种子以保证可重复性\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # 参数设置\n",
    "    # 源语言词表大小，包括特殊token（如<PAD>, <START>, <END>等）\n",
    "    src_vocab_size = 12  \n",
    "    # 目标语言词表大小，包括特殊token\n",
    "    tgt_vocab_size = 12\n",
    "    # Transformer模型的维度，决定了词嵌入和各层的特征维度\n",
    "    d_model = 128\n",
    "    # 多头注意力机制中的头数，必须能被d_model整除\n",
    "    num_heads = 8\n",
    "    # Transformer编码器和解码器的层数\n",
    "    num_layers = 3\n",
    "    # 前馈神经网络的隐藏层维度\n",
    "    d_ff = 512\n",
    "    # Dropout比率，用于防止过拟合\n",
    "    dropout = 0.1\n",
    "    # 每个批次的样本数\n",
    "    batch_size = 32\n",
    "    # 训练的轮数\n",
    "    num_epochs = 10\n",
    "    # 训练设备，优先使用GPU，否则使用CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 创建数据集和数据加载器\n",
    "    dataset = SimpleDataset()\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # 创建模型\n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params}\")\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # 训练模型\n",
    "    train_model(model, train_loader, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "    # 测试示例\n",
    "    test_input = [3, 1, 4, 1, 5]\n",
    "    print(f\"Input sequence: {test_input}\")\n",
    "    output = predict(model, test_input, device)\n",
    "    print(f\"Predicted sequence: {output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0, 20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
