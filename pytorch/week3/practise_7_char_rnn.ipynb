{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5392\n",
      "Epoch 20, Loss: 2.1224\n",
      "Epoch 30, Loss: 1.6201\n",
      "Epoch 40, Loss: 1.2897\n",
      "Epoch 50, Loss: 1.0433\n",
      "Epoch 60, Loss: 0.8437\n",
      "Epoch 70, Loss: 0.6600\n",
      "Epoch 80, Loss: 0.5161\n",
      "Epoch 90, Loss: 0.4118\n",
      "Epoch 100, Loss: 0.3368\n",
      "训练完成\n",
      "生成的文本:\n",
      "春眠不觉晓，\n",
      "    处眠闻啼鸟。\n",
      "    夜来风雨声，\n",
      "    夜来风雨声。\n",
      "    花落知多鸟。   来处\n",
      "声，\n",
      "    夜落知觉声，\n",
      "    花来风雨声，\n",
      "    夜落知声，，\n",
      "    花闻，\n",
      " \n",
      "  夜\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size,n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        output, hidden = self.rnn(x,hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "\n",
    "def process_data(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {\n",
    "        ch: i for i, ch in enumerate(chars)\n",
    "    }\n",
    "    idx_to_char = {\n",
    "        i: ch for i, ch in enumerate(chars)\n",
    "    }\n",
    "    return chars, char_to_idx, idx_to_char\n",
    "\n",
    "def create_sequences(text, char_to_idx, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(0, len(text) - seq_length):\n",
    "        sequence = text[i:i+seq_length]\n",
    "        target = text[i+1:i+seq_length+1]\n",
    "        \n",
    "        x.append([char_to_idx[char] for char in sequence])\n",
    "        y.append([char_to_idx[char] for char in target])\n",
    "        \n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    return x, y\n",
    "\n",
    "def train_model(model, data, targets, criterion, optimizer, vocab_size, batch_size=32):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 修正：正确计算批次数量\n",
    "    n_batches = len(data) // batch_size\n",
    "    if n_batches == 0:  # 确保至少有一个批次\n",
    "        n_batches = 1\n",
    "        batch_size = len(data)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(data))  # 防止越界\n",
    "        batch_data = data[start:end]\n",
    "        batch_targets = targets[start:end]\n",
    "        \n",
    "        current_batch_size = len(batch_data)  # 获取实际批次大小\n",
    "        \n",
    "        # 使用实际批次大小创建张量\n",
    "        x = torch.zeros(current_batch_size, len(batch_data[0]), vocab_size)\n",
    "        for i, sequence in enumerate(batch_data):\n",
    "            for t, char_idx in enumerate(sequence):\n",
    "                x[i, t, char_idx] = 1\n",
    "                \n",
    "        hidden = model.init_hidden(current_batch_size)\n",
    "        \n",
    "        output, hidden = model(x, hidden)\n",
    "        \n",
    "        loss = criterion(output.view(-1, vocab_size), batch_targets.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def generate_text(model, initial_str, char_to_idx, idx_to_char, vocab_size, pred_length=100):\n",
    "    model.eval()\n",
    "    current_str = initial_str\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(pred_length):\n",
    "            x = torch.zeros(1, 1, vocab_size)\n",
    "            x[0,0,char_to_idx[current_str[-1]]] = 1\n",
    "            \n",
    "            output, hidden = model(x,hidden)\n",
    "            \n",
    "            probs = torch.softmax(output[0,-1], dim=0).detach().numpy()\n",
    "            char_idx = torch.multinomial(torch.tensor(probs), num_samples=1).item()\n",
    "            \n",
    "            current_str += idx_to_char[char_idx]\n",
    "            \n",
    "    return current_str\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    text = \"\"\"\n",
    "    春眠不觉晓，\n",
    "    处处闻啼鸟。\n",
    "    夜来风雨声，\n",
    "    花落知多少。\n",
    "    \"\"\"\n",
    "    \n",
    "    chars, char_to_idx, idx_to_char = process_data(text)\n",
    "    \n",
    "    seq_length = 10\n",
    "    \n",
    "    x,y = create_sequences(text,char_to_idx=char_to_idx, seq_length=seq_length)\n",
    "    \n",
    "    input_size = len(chars)\n",
    "    hidden_size = 128\n",
    "    n_layers = 2\n",
    "    \n",
    "    # 创建模型\n",
    "    model = CharRNN(input_size=input_size, hidden_size=hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    n_epochs = 100\n",
    "    vocab_size = len(chars)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        loss = train_model(model,x,y,criterion,optimizer,vocab_size)\n",
    "        if(epoch + 1)%10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "            \n",
    "    print(\"训练完成\")\n",
    "    \n",
    "    initial_str = \"春眠不觉晓\"\n",
    "    generated_text = generate_text(model, initial_str, char_to_idx, idx_to_char, vocab_size)\n",
    "    print(f\"生成的文本:\\n{generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
