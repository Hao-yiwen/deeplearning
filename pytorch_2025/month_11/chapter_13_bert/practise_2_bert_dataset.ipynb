{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675726d2",
   "metadata": {},
   "source": [
    "# 用于预训练BERT的数据集\n",
    "\n",
    "## BERT预训练的两个核心任务\n",
    "\n",
    "BERT（Bidirectional Encoder Representations from Transformers）预训练包含两个任务：\n",
    "\n",
    "### 1. 遮蔽语言模型 (MLM - Masked Language Model)\n",
    "- **目的**：让模型学习双向上下文表示\n",
    "- **做法**：随机遮蔽输入中15%的词元，让模型预测被遮蔽的词\n",
    "- **遮蔽策略**：\n",
    "  - 80% 替换为 `<mask>` \n",
    "  - 10% 替换为随机词\n",
    "  - 10% 保持不变\n",
    "\n",
    "### 2. 下一句预测 (NSP - Next Sentence Prediction)\n",
    "- **目的**：让模型理解句子之间的关系\n",
    "- **做法**：给定句子A和句子B，预测B是否是A的下一句\n",
    "- **数据构造**：50%是真实的下一句，50%是随机句子\n",
    "\n",
    "## 数据流程图\n",
    "\n",
    "```\n",
    "原始文本 → 分段落 → 生成句子对(NSP) → 遮蔽词元(MLM) → 填充对齐 → DataLoader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5572eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5447f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki(data_dir):\n",
    "    \"\"\"\n",
    "    读取WikiText-2数据集并预处理成段落格式\n",
    "    \n",
    "    WikiText-2数据集结构：\n",
    "    - 每一行是一个段落\n",
    "    - 段落内的句子用 \" . \" 分隔\n",
    "    \n",
    "    返回值结构：\n",
    "    paragraphs = [\n",
    "        [\"sentence1\", \"sentence2\", ...],  # 段落1\n",
    "        [\"sentence1\", \"sentence2\", ...],  # 段落2\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    paragraphs = []\n",
    "    for line in lines:\n",
    "        # 按 \" . \" 分割成句子列表\n",
    "        sentences = line.strip().lower().split(' . ')\n",
    "        # 只保留包含至少2个句子的段落（NSP任务需要句子对）\n",
    "        if len(sentences) >= 2:\n",
    "            paragraphs.append(sentences)\n",
    "    \n",
    "    # 打乱段落顺序，增加数据随机性\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvp2app994i",
   "metadata": {},
   "source": [
    "## 第一步：读取和预处理数据\n",
    "\n",
    "WikiText-2是一个从Wikipedia提取的语言建模数据集，包含约200万个词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c100993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    \"\"\"\n",
    "    为NSP任务生成句子对\n",
    "    \n",
    "    参数：\n",
    "        sentence: 当前句子（句子A）\n",
    "        next_sentence: 原本的下一个句子\n",
    "        paragraphs: 所有段落，用于采样负样本\n",
    "    \n",
    "    返回：\n",
    "        sentence: 句子A\n",
    "        next_sentence: 句子B（可能是真实下一句或随机句子）\n",
    "        is_next: 布尔值，True表示是真实的下一句\n",
    "    \n",
    "    策略：50%概率返回真实下一句，50%概率返回随机句子\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        # 正样本：使用真实的下一个句子\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 负样本：从随机段落中随机选一个句子\n",
    "        # random.choice(paragraphs) -> 随机选一个段落\n",
    "        # random.choice(...) -> 从该段落中随机选一个句子\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bxcp9l1wuus",
   "metadata": {},
   "source": [
    "## 第二步：生成下一句预测(NSP)任务数据\n",
    "\n",
    "NSP任务的目标是判断两个句子是否相邻。训练数据构造方式：\n",
    "- **正样本(is_next=True)**：取相邻的两个句子\n",
    "- **负样本(is_next=False)**：取一个句子 + 随机句子\n",
    "\n",
    "BERT输入格式：`<cls> 句子A <sep> 句子B <sep>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1e015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    \"\"\"\n",
    "    从一个段落生成所有NSP训练样本\n",
    "    \n",
    "    参数：\n",
    "        paragraph: 当前段落（句子列表，每个句子是词元列表）\n",
    "        paragraphs: 所有段落（用于采样负样本）\n",
    "        vocab: 词表（此函数中未使用，但保持接口一致）\n",
    "        max_len: 最大序列长度\n",
    "    \n",
    "    返回：\n",
    "        nsp_data_from_paragraph: [(tokens, segments, is_next), ...]\n",
    "        - tokens: ['<cls>', 词1, 词2, ..., '<sep>', 词1, 词2, ..., '<sep>']\n",
    "        - segments: [0, 0, ..., 0, 1, 1, ..., 1] 表示属于句子A还是句子B\n",
    "        - is_next: True/False\n",
    "    \"\"\"\n",
    "    nsp_data_from_paragraph = []\n",
    "    \n",
    "    # 遍历段落中的相邻句子对\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        # 生成句子对（可能是正样本或负样本）\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        \n",
    "        # 检查长度：tokens_a + tokens_b + 3个特殊词元(<cls>, <sep>, <sep>)\n",
    "        # 如果超过max_len则跳过这个样本\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        \n",
    "        # 使用d2l工具函数构造BERT输入格式\n",
    "        # tokens: ['<cls>'] + tokens_a + ['<sep>'] + tokens_b + ['<sep>']\n",
    "        # segments: [0]*len(句子A部分) + [1]*len(句子B部分)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    \n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290b0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    \"\"\"\n",
    "    对tokens进行遮蔽处理，生成MLM任务的输入和标签\n",
    "    \n",
    "    参数：\n",
    "        tokens: 原始词元列表 ['<cls>', 'hello', 'world', '<sep>', ...]\n",
    "        candidate_pred_positions: 可以被遮蔽的位置索引列表（不包括特殊词元）\n",
    "        num_mlm_preds: 需要遮蔽的词元数量（约15%）\n",
    "        vocab: 词表（用于随机替换时采样）\n",
    "    \n",
    "    返回：\n",
    "        mlm_input_tokens: 遮蔽后的词元列表（用于模型输入）\n",
    "        pred_positions_and_labels: [(位置, 原始词元), ...] 用于计算损失\n",
    "    \"\"\"\n",
    "    # 复制一份tokens，避免修改原始数据\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    \n",
    "    # 随机打乱候选位置，然后选取前num_mlm_preds个\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    \n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        # 已选够需要预测的数量，退出循环\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        \n",
    "        masked_token = None\n",
    "        rand = random.random()\n",
    "        \n",
    "        if rand < 0.8:\n",
    "            # 80%概率：替换为<mask>词元\n",
    "            masked_token = '<mask>'\n",
    "        elif rand < 0.9:\n",
    "            # 10%概率：保持原词不变（但仍然要预测）\n",
    "            masked_token = tokens[mlm_pred_position]\n",
    "        else:\n",
    "            # 10%概率：替换为词表中的随机词\n",
    "            masked_token = random.choice(vocab.idx_to_token)\n",
    "        \n",
    "        # 在输入中进行替换\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 记录位置和原始词元（作为预测标签）\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    \n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ijqtqtdlsz",
   "metadata": {},
   "source": [
    "## 第三步：生成遮蔽语言模型(MLM)任务数据\n",
    "\n",
    "MLM任务的核心思想是\"完形填空\"：随机遮蔽一些词，让模型根据上下文预测被遮蔽的词。\n",
    "\n",
    "### 遮蔽策略（针对被选中的15%词元）：\n",
    "| 概率 | 处理方式 | 原因 |\n",
    "|------|----------|------|\n",
    "| 80% | 替换为`<mask>` | 主要的遮蔽方式 |\n",
    "| 10% | 保持不变 | 避免模型认为`<mask>`一定要预测 |\n",
    "| 10% | 替换为随机词 | 增加鲁棒性 |\n",
    "\n",
    "这种策略可以避免预训练和微调之间的不匹配问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69198e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    \"\"\"\n",
    "    从词元序列生成MLM任务的完整数据\n",
    "    \n",
    "    参数：\n",
    "        tokens: 词元列表 ['<cls>', 'the', 'cat', '<sep>', 'sat', '<sep>']\n",
    "        vocab: 词表对象\n",
    "    \n",
    "    返回：\n",
    "        token_ids: 遮蔽后的词元ID列表（模型输入）\n",
    "        pred_positions: 被遮蔽的位置列表\n",
    "        mlm_pred_label_ids: 被遮蔽位置的原始词元ID（预测标签）\n",
    "    \n",
    "    示例：\n",
    "        输入tokens: ['<cls>', 'the', 'cat', 'sat', '<sep>']\n",
    "        假设遮蔽位置2(cat)和3(sat)\n",
    "        输出:\n",
    "            token_ids: [cls_id, the_id, mask_id, random_id, sep_id]\n",
    "            pred_positions: [2, 3]\n",
    "            mlm_pred_label_ids: [cat_id, sat_id]\n",
    "    \"\"\"\n",
    "    # 找出所有可以被遮蔽的位置（排除特殊词元）\n",
    "    candidate_pred_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # <cls>和<sep>是特殊词元，不参与MLM预测\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    \n",
    "    # 计算需要遮蔽的词元数量：约15%，但至少1个\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    \n",
    "    # 执行遮蔽操作\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    \n",
    "    # 按位置排序，保证顺序一致性\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "    \n",
    "    # 分离位置和标签\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    \n",
    "    # 将词元转换为ID（vocab[tokens]会调用vocab的__getitem__方法）\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f562028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    \"\"\"\n",
    "    将所有样本填充到相同长度，转换为PyTorch张量\n",
    "    \n",
    "    参数：\n",
    "        examples: [(token_ids, pred_positions, mlm_labels, segments, is_next), ...]\n",
    "        max_len: 最大序列长度\n",
    "        vocab: 词表对象\n",
    "    \n",
    "    返回（7个列表，每个列表包含所有样本的对应数据）：\n",
    "        all_token_ids: 填充后的输入序列 [batch, max_len]\n",
    "        all_segments: 段落标识 [batch, max_len]\n",
    "        valid_lens: 有效长度（不含<pad>）[batch]\n",
    "        all_pred_positions: MLM预测位置 [batch, max_num_mlm_preds]\n",
    "        all_mlm_weights: MLM损失权重 [batch, max_num_mlm_preds]\n",
    "        all_mlm_labels: MLM预测标签 [batch, max_num_mlm_preds]\n",
    "        nsp_labels: NSP标签 [batch]\n",
    "    \"\"\"\n",
    "    # MLM预测的最大数量 = 序列长度 * 15%\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    \n",
    "    # 初始化存储列表\n",
    "    all_token_ids, all_segments, valid_lens = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    \n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # ===== 1. 填充token_ids到max_len =====\n",
    "        # 在序列末尾添加<pad>词元\n",
    "        padding_len = max_len - len(token_ids)\n",
    "        all_token_ids.append(torch.tensor(\n",
    "            token_ids + [vocab['<pad>']] * padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 2. 填充segments到max_len =====\n",
    "        # 填充部分的segment标识为0\n",
    "        all_segments.append(torch.tensor(\n",
    "            segments + [0] * padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 3. 记录有效长度（用于attention mask）=====\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        \n",
    "        # ===== 4. 填充pred_positions到max_num_mlm_preds =====\n",
    "        mlm_padding_len = max_num_mlm_preds - len(pred_positions)\n",
    "        all_pred_positions.append(torch.tensor(\n",
    "            pred_positions + [0] * mlm_padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 5. 设置mlm_weights（真实预测位置权重为1，填充位置权重为0）=====\n",
    "        # 这样在计算损失时，填充位置的损失会被过滤掉\n",
    "        all_mlm_weights.append(torch.tensor(\n",
    "            [1.0] * len(mlm_pred_label_ids) + [0.0] * mlm_padding_len,\n",
    "            dtype=torch.float32))\n",
    "        \n",
    "        # ===== 6. 填充mlm_labels =====\n",
    "        all_mlm_labels.append(torch.tensor(\n",
    "            mlm_pred_label_ids + [0] * mlm_padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 7. NSP标签（True->1, False->0）=====\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    \n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bxc4skg4zb",
   "metadata": {},
   "source": [
    "## 第四步：填充数据到固定长度\n",
    "\n",
    "由于不同样本的序列长度不同，需要将它们填充到相同长度才能批量处理。\n",
    "\n",
    "### 需要填充的内容：\n",
    "| 数据 | 填充值 | 说明 |\n",
    "|------|--------|------|\n",
    "| token_ids | `<pad>` | 输入序列填充 |\n",
    "| segments | 0 | 段落标识填充 |\n",
    "| pred_positions | 0 | MLM预测位置填充 |\n",
    "| mlm_weights | 0.0 | 填充位置的损失权重为0 |\n",
    "| mlm_labels | 0 | MLM标签填充 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d92b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    WikiText-2 BERT预训练数据集\n",
    "    \n",
    "    数据处理流程：\n",
    "    1. 分词：将句子字符串转换为词元列表\n",
    "    2. 构建词表：基于所有句子构建词表\n",
    "    3. 生成NSP数据：为每个段落生成句子对\n",
    "    4. 生成MLM数据：对每个句子对进行遮蔽处理\n",
    "    5. 填充对齐：将所有样本填充到相同长度\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            paragraphs: 段落列表，每个段落是句子字符串列表\n",
    "                       [[\"sentence1\", \"sentence2\"], [\"sentence3\", \"sentence4\"], ...]\n",
    "            max_len: 最大序列长度\n",
    "        \"\"\"\n",
    "        # ===== 步骤1: 分词 =====\n",
    "        # 将每个句子从字符串转换为词元列表\n",
    "        # 输入: [[\"hello world\", \"foo bar\"], ...]\n",
    "        # 输出: [[[\"hello\", \"world\"], [\"foo\", \"bar\"]], ...]\n",
    "        paragraphs = [d2l.tokenize(paragraph, token='word') \n",
    "                      for paragraph in paragraphs]\n",
    "        \n",
    "        # ===== 步骤2: 构建词表 =====\n",
    "        # 将所有句子展平成一个列表用于构建词表\n",
    "        sentences = [sentence for paragraph in paragraphs \n",
    "                     for sentence in paragraph]\n",
    "        # min_freq=5: 词频少于5的词会被替换为<unk>\n",
    "        # reserved_tokens: 预留的特殊词元\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>',   # 填充词元\n",
    "            '<mask>',  # MLM遮蔽词元\n",
    "            '<cls>',   # 句子开头词元\n",
    "            '<sep>'    # 句子分隔词元\n",
    "        ])\n",
    "        \n",
    "        # ===== 步骤3: 生成NSP数据 =====\n",
    "        # 为每个段落生成所有可能的句子对\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        \n",
    "        # ===== 步骤4: 生成MLM数据 =====\n",
    "        # 对每个句子对进行遮蔽处理\n",
    "        # 输入: (tokens, segments, is_next)\n",
    "        # 输出: (token_ids, pred_positions, mlm_labels, segments, is_next)\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                     + (segments, is_next))\n",
    "                    for tokens, segments, is_next in examples]\n",
    "        \n",
    "        # ===== 步骤5: 填充到固定长度 =====\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
    "            examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"返回第idx个样本的所有数据\"\"\"\n",
    "        return (self.all_token_ids[idx],      # 输入token IDs\n",
    "                self.all_segments[idx],        # 段落标识(0或1)\n",
    "                self.valid_lens[idx],          # 有效长度\n",
    "                self.all_pred_positions[idx],  # MLM预测位置\n",
    "                self.all_mlm_weights[idx],     # MLM损失权重\n",
    "                self.all_mlm_labels[idx],      # MLM预测标签\n",
    "                self.nsp_labels[idx])          # NSP标签(0或1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4pauaxp0u",
   "metadata": {},
   "source": [
    "## 第五步：创建PyTorch Dataset\n",
    "\n",
    "将所有数据处理步骤整合到一个Dataset类中，便于与DataLoader配合使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820f77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"\n",
    "    加载WikiText-2数据集用于BERT预训练\n",
    "    \n",
    "    参数：\n",
    "        batch_size: 批量大小\n",
    "        max_len: 最大序列长度（包含特殊词元）\n",
    "    \n",
    "    返回：\n",
    "        train_iter: 训练数据迭代器\n",
    "        vocab: 词表对象\n",
    "    \"\"\"\n",
    "    # 使用本地已解压的数据目录\n",
    "    data_dir = '../data/wikitext-2'\n",
    "    \n",
    "    # 读取并预处理数据\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    # 注意：在Jupyter notebook中需要设置num_workers=0\n",
    "    # 因为notebook中定义的类无法被pickle序列化用于多进程\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        train_set, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0  # notebook环境下必须为0\n",
    "    )\n",
    "    \n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unwu70myj2j",
   "metadata": {},
   "source": [
    "## 第六步：数据加载器\n",
    "\n",
    "将数据集包装成DataLoader，支持批量加载和多进程预取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f015313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_X:        torch.Size([512, 64])\n",
      "segments_X:      torch.Size([512, 64])\n",
      "valid_lens_x:    torch.Size([512])\n",
      "pred_positions_X:torch.Size([512, 10])\n",
      "mlm_weights_X:   torch.Size([512, 10])\n",
      "mlm_Y:           torch.Size([512, 10])\n",
      "nsp_y:           torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "batch_size = 512  # 批量大小\n",
    "max_len = 64      # 最大序列长度\n",
    "\n",
    "# 加载数据\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "# 查看第一个batch的形状\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, \n",
    "     mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    \n",
    "    print(f\"tokens_X:        {tokens_X.shape}\")        # [512, 64] - 输入序列\n",
    "    print(f\"segments_X:      {segments_X.shape}\")      # [512, 64] - 段落标识\n",
    "    print(f\"valid_lens_x:    {valid_lens_x.shape}\")    # [512] - 有效长度\n",
    "    print(f\"pred_positions_X:{pred_positions_X.shape}\")# [512, 10] - MLM位置 (64*0.15≈10)\n",
    "    print(f\"mlm_weights_X:   {mlm_weights_X.shape}\")   # [512, 10] - MLM权重\n",
    "    print(f\"mlm_Y:           {mlm_Y.shape}\")           # [512, 10] - MLM标签\n",
    "    print(f\"nsp_y:           {nsp_y.shape}\")           # [512] - NSP标签\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ykphl6s1t",
   "metadata": {},
   "source": [
    "## 测试数据加载\n",
    "\n",
    "验证数据加载器是否正常工作，并查看各张量的形状。\n",
    "\n",
    "### 输出形状说明：\n",
    "| 张量 | 形状 | 说明 |\n",
    "|------|------|------|\n",
    "| tokens_X | [batch, max_len] | 输入token IDs |\n",
    "| segments_X | [batch, max_len] | 段落标识 |\n",
    "| valid_lens_x | [batch] | 每个样本的有效长度 |\n",
    "| pred_positions_X | [batch, max_mlm_preds] | MLM预测位置 |\n",
    "| mlm_weights_X | [batch, max_mlm_preds] | MLM损失权重 |\n",
    "| mlm_Y | [batch, max_mlm_preds] | MLM预测标签 |\n",
    "| nsp_y | [batch] | NSP标签 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72bf823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小: 20066\n"
     ]
    }
   ],
   "source": [
    "# 查看词表大小\n",
    "print(f\"词表大小: {len(vocab)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
