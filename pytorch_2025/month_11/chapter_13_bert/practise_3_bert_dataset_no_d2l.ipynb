{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675726d2",
   "metadata": {},
   "source": [
    "# 预训练BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5572eb",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport random\nimport time\nimport torch\nfrom torch import nn\nimport collections\nimport math\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\n# ===============================\n# 纯原生实现：替代d2l库的所有功能\n# ===============================\n\ndef tokenize(lines, token='word'):\n    \"\"\"\n    将文本行分词为单词或字符\n    \n    参数：\n        lines: 字符串列表，每个字符串是一行文本\n        token: 'word' 或 'char'，分词粒度\n    \n    返回：\n        tokens: 二维列表，每个元素是一行的词元列表\n    \"\"\"\n    if token == 'word':\n        return [line.split() for line in lines]\n    elif token == 'char':\n        return [list(line) for line in lines]\n    else:\n        raise ValueError(f\"未知的token类型: {token}\")\n\n\nclass Vocab:\n    \"\"\"\n    词表类：实现词元到索引的双向映射\n    \n    特殊词元：\n        <unk>: 未知词元，索引为0\n        其他reserved_tokens按顺序添加\n    \"\"\"\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        \"\"\"\n        参数：\n            tokens: 二维列表，所有句子的词元\n            min_freq: 最小词频，低于此频率的词元将被忽略\n            reserved_tokens: 预留的特殊词元列表\n        \"\"\"\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        \n        # 统计词频\n        counter = collections.Counter()\n        for line in tokens:\n            for token in line:\n                counter[token] += 1\n        \n        # 按词频排序\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n        \n        # 构建词表\n        # <unk>的索引为0\n        self.idx_to_token = ['<unk>'] + reserved_tokens\n        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n        \n        # 添加满足最小词频的词元\n        for token, freq in self._token_freqs:\n            if freq < min_freq:\n                break\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n    \n    def __len__(self):\n        return len(self.idx_to_token)\n    \n    def __getitem__(self, tokens):\n        \"\"\"将词元转换为索引\"\"\"\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.token_to_idx['<unk>'])\n        return [self.__getitem__(token) for token in tokens]\n    \n    def to_tokens(self, indices):\n        \"\"\"将索引转换为词元\"\"\"\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[idx] for idx in indices]\n\n\ndef get_tokens_and_segments(tokens_a, tokens_b=None):\n    \"\"\"\n    构造BERT的输入格式\n    \n    格式：<cls> 句子A <sep> [句子B <sep>]\n    \n    参数：\n        tokens_a: 句子A的词元列表\n        tokens_b: 句子B的词元列表（可选）\n    \n    返回：\n        tokens: 完整的词元列表\n        segments: 段落标识列表（0表示句子A，1表示句子B）\n    \"\"\"\n    tokens = ['<cls>'] + tokens_a + ['<sep>']\n    # 句子A部分的segment为0\n    segments = [0] * (len(tokens_a) + 2)\n    \n    if tokens_b is not None:\n        tokens = tokens + tokens_b + ['<sep>']\n        # 句子B部分的segment为1\n        segments = segments + [1] * (len(tokens_b) + 1)\n    \n    return tokens, segments\n\n\ndef masked_softmax(X, valid_lens):\n    \"\"\"\n    带掩码的softmax操作\n    \n    参数：\n        X: 形状为(batch_size, num_queries, num_keys)的张量\n        valid_lens: 有效长度，形状为(batch_size,)或(batch_size, num_queries)\n    \n    返回：\n        softmax后的注意力权重，无效位置的权重为0\n    \"\"\"\n    if valid_lens is None:\n        return nn.functional.softmax(X, dim=-1)\n    else:\n        shape = X.shape\n        if valid_lens.dim() == 1:\n            # valid_lens形状为(batch_size,)，扩展到每个query\n            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n        else:\n            # valid_lens形状为(batch_size, num_queries)\n            valid_lens = valid_lens.reshape(-1)\n        \n        # 将X展平为(batch_size * num_queries, num_keys)\n        X = X.reshape(-1, shape[-1])\n        \n        # 创建掩码：超过有效长度的位置设为很大的负数\n        max_len = X.shape[1]\n        mask = torch.arange(max_len, dtype=torch.float32, device=X.device)[None, :] < valid_lens[:, None]\n        X[~mask] = -1e6\n        \n        return nn.functional.softmax(X.reshape(shape), dim=-1)\n\n\nclass DotProductAttention(nn.Module):\n    \"\"\"\n    缩放点积注意力\n    \n    公式：Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n    \"\"\"\n    def __init__(self, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, queries, keys, values, valid_lens=None):\n        \"\"\"\n        参数：\n            queries: (batch_size, num_queries, d)\n            keys: (batch_size, num_keys, d)\n            values: (batch_size, num_keys, d_v)\n            valid_lens: (batch_size,) 或 (batch_size, num_queries)\n        \n        返回：\n            output: (batch_size, num_queries, d_v)\n        \"\"\"\n        d = queries.shape[-1]\n        # 计算注意力分数: (batch_size, num_queries, num_keys)\n        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n        # 应用掩码softmax\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # 计算输出\n        return torch.bmm(self.dropout(self.attention_weights), values)\n\n\ndef try_all_gpus():\n    \"\"\"获取所有可用的GPU，如果没有GPU则返回CPU\"\"\"\n    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n    return devices if devices else [torch.device('cpu')]\n\n\nclass Timer:\n    \"\"\"记录多次运行时间\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start_time = None\n    \n    def start(self):\n        \"\"\"启动计时器\"\"\"\n        self.start_time = time.time()\n    \n    def stop(self):\n        \"\"\"停止计时器并记录时间\"\"\"\n        self.times.append(time.time() - self.start_time)\n        return self.times[-1]\n    \n    def avg(self):\n        \"\"\"返回平均时间\"\"\"\n        return sum(self.times) / len(self.times)\n    \n    def sum(self):\n        \"\"\"返回总时间\"\"\"\n        return sum(self.times)\n    \n    def cumsum(self):\n        \"\"\"返回累计时间\"\"\"\n        return list(itertools.accumulate(self.times))\n\n\nclass Animator:\n    \"\"\"在动画中绘制数据（用于训练可视化）\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        if legend is None:\n            legend = []\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes]\n        self.config_axes = lambda: self.set_axes(\n            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n    \n    def set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n        \"\"\"设置matplotlib的轴\"\"\"\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n        ax.set_xscale(xscale)\n        ax.set_yscale(yscale)\n        ax.set_xlim(xlim)\n        ax.set_ylim(ylim)\n        if legend:\n            ax.legend(legend)\n        ax.grid()\n    \n    def add(self, x, y):\n        \"\"\"向图表中添加数据点\"\"\"\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n    \n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n    \n    def reset(self):\n        self.data = [0.0] * len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki(data_dir):\n",
    "    \"\"\"\n",
    "    读取WikiText-2数据集并预处理成段落格式\n",
    "    \n",
    "    WikiText-2数据集结构：\n",
    "    - 每一行是一个段落\n",
    "    - 段落内的句子用 \" . \" 分隔\n",
    "    \n",
    "    返回值结构：\n",
    "    paragraphs = [\n",
    "        [\"sentence1\", \"sentence2\", ...],  # 段落1\n",
    "        [\"sentence1\", \"sentence2\", ...],  # 段落2\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    paragraphs = []\n",
    "    for line in lines:\n",
    "        # 按 \" . \" 分割成句子列表\n",
    "        sentences = line.strip().lower().split(' . ')\n",
    "        # 只保留包含至少2个句子的段落（NSP任务需要句子对）\n",
    "        if len(sentences) >= 2:\n",
    "            paragraphs.append(sentences)\n",
    "    \n",
    "    # 打乱段落顺序，增加数据随机性\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvp2app994i",
   "metadata": {},
   "source": [
    "## 第一步：读取和预处理数据\n",
    "\n",
    "WikiText-2是一个从Wikipedia提取的语言建模数据集，包含约200万个词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c100993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    \"\"\"\n",
    "    为NSP任务生成句子对\n",
    "    \n",
    "    参数：\n",
    "        sentence: 当前句子（句子A）\n",
    "        next_sentence: 原本的下一个句子\n",
    "        paragraphs: 所有段落，用于采样负样本\n",
    "    \n",
    "    返回：\n",
    "        sentence: 句子A\n",
    "        next_sentence: 句子B（可能是真实下一句或随机句子）\n",
    "        is_next: 布尔值，True表示是真实的下一句\n",
    "    \n",
    "    策略：50%概率返回真实下一句，50%概率返回随机句子\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        # 正样本：使用真实的下一个句子\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 负样本：从随机段落中随机选一个句子\n",
    "        # random.choice(paragraphs) -> 随机选一个段落\n",
    "        # random.choice(...) -> 从该段落中随机选一个句子\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bxcp9l1wuus",
   "metadata": {},
   "source": [
    "## 第二步：生成下一句预测(NSP)任务数据\n",
    "\n",
    "NSP任务的目标是判断两个句子是否相邻。训练数据构造方式：\n",
    "- **正样本(is_next=True)**：取相邻的两个句子\n",
    "- **负样本(is_next=False)**：取一个句子 + 随机句子\n",
    "\n",
    "BERT输入格式：`<cls> 句子A <sep> 句子B <sep>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e015c",
   "metadata": {},
   "outputs": [],
   "source": "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n    \"\"\"\n    从一个段落生成所有NSP训练样本\n    \n    参数：\n        paragraph: 当前段落（句子列表，每个句子是词元列表）\n        paragraphs: 所有段落（用于采样负样本）\n        vocab: 词表（此函数中未使用，但保持接口一致）\n        max_len: 最大序列长度\n    \n    返回：\n        nsp_data_from_paragraph: [(tokens, segments, is_next), ...]\n        - tokens: ['<cls>', 词1, 词2, ..., '<sep>', 词1, 词2, ..., '<sep>']\n        - segments: [0, 0, ..., 0, 1, 1, ..., 1] 表示属于句子A还是句子B\n        - is_next: True/False\n    \"\"\"\n    nsp_data_from_paragraph = []\n    \n    # 遍历段落中的相邻句子对\n    for i in range(len(paragraph) - 1):\n        # 生成句子对（可能是正样本或负样本）\n        tokens_a, tokens_b, is_next = _get_next_sentence(\n            paragraph[i], paragraph[i + 1], paragraphs)\n        \n        # 检查长度：tokens_a + tokens_b + 3个特殊词元(<cls>, <sep>, <sep>)\n        # 如果超过max_len则跳过这个样本\n        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n            continue\n        \n        # 使用自定义函数构造BERT输入格式（替代d2l.get_tokens_and_segments）\n        # tokens: ['<cls>'] + tokens_a + ['<sep>'] + tokens_b + ['<sep>']\n        # segments: [0]*len(句子A部分) + [1]*len(句子B部分)\n        tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n        nsp_data_from_paragraph.append((tokens, segments, is_next))\n    \n    return nsp_data_from_paragraph"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    \"\"\"\n",
    "    对tokens进行遮蔽处理，生成MLM任务的输入和标签\n",
    "    \n",
    "    参数：\n",
    "        tokens: 原始词元列表 ['<cls>', 'hello', 'world', '<sep>', ...]\n",
    "        candidate_pred_positions: 可以被遮蔽的位置索引列表（不包括特殊词元）\n",
    "        num_mlm_preds: 需要遮蔽的词元数量（约15%）\n",
    "        vocab: 词表（用于随机替换时采样）\n",
    "    \n",
    "    返回：\n",
    "        mlm_input_tokens: 遮蔽后的词元列表（用于模型输入）\n",
    "        pred_positions_and_labels: [(位置, 原始词元), ...] 用于计算损失\n",
    "    \"\"\"\n",
    "    # 复制一份tokens，避免修改原始数据\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    \n",
    "    # 随机打乱候选位置，然后选取前num_mlm_preds个\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    \n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        # 已选够需要预测的数量，退出循环\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        \n",
    "        masked_token = None\n",
    "        rand = random.random()\n",
    "        \n",
    "        if rand < 0.8:\n",
    "            # 80%概率：替换为<mask>词元\n",
    "            masked_token = '<mask>'\n",
    "        elif rand < 0.9:\n",
    "            # 10%概率：保持原词不变（但仍然要预测）\n",
    "            masked_token = tokens[mlm_pred_position]\n",
    "        else:\n",
    "            # 10%概率：替换为词表中的随机词\n",
    "            masked_token = random.choice(vocab.idx_to_token)\n",
    "        \n",
    "        # 在输入中进行替换\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 记录位置和原始词元（作为预测标签）\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    \n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ijqtqtdlsz",
   "metadata": {},
   "source": [
    "## 第三步：生成遮蔽语言模型(MLM)任务数据\n",
    "\n",
    "MLM任务的核心思想是\"完形填空\"：随机遮蔽一些词，让模型根据上下文预测被遮蔽的词。\n",
    "\n",
    "### 遮蔽策略（针对被选中的15%词元）：\n",
    "| 概率 | 处理方式 | 原因 |\n",
    "|------|----------|------|\n",
    "| 80% | 替换为`<mask>` | 主要的遮蔽方式 |\n",
    "| 10% | 保持不变 | 避免模型认为`<mask>`一定要预测 |\n",
    "| 10% | 替换为随机词 | 增加鲁棒性 |\n",
    "\n",
    "这种策略可以避免预训练和微调之间的不匹配问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69198e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    \"\"\"\n",
    "    从词元序列生成MLM任务的完整数据\n",
    "    \n",
    "    参数：\n",
    "        tokens: 词元列表 ['<cls>', 'the', 'cat', '<sep>', 'sat', '<sep>']\n",
    "        vocab: 词表对象\n",
    "    \n",
    "    返回：\n",
    "        token_ids: 遮蔽后的词元ID列表（模型输入）\n",
    "        pred_positions: 被遮蔽的位置列表\n",
    "        mlm_pred_label_ids: 被遮蔽位置的原始词元ID（预测标签）\n",
    "    \n",
    "    示例：\n",
    "        输入tokens: ['<cls>', 'the', 'cat', 'sat', '<sep>']\n",
    "        假设遮蔽位置2(cat)和3(sat)\n",
    "        输出:\n",
    "            token_ids: [cls_id, the_id, mask_id, random_id, sep_id]\n",
    "            pred_positions: [2, 3]\n",
    "            mlm_pred_label_ids: [cat_id, sat_id]\n",
    "    \"\"\"\n",
    "    # 找出所有可以被遮蔽的位置（排除特殊词元）\n",
    "    candidate_pred_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # <cls>和<sep>是特殊词元，不参与MLM预测\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    \n",
    "    # 计算需要遮蔽的词元数量：约15%，但至少1个\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    \n",
    "    # 执行遮蔽操作\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    \n",
    "    # 按位置排序，保证顺序一致性\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "    \n",
    "    # 分离位置和标签\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    \n",
    "    # 将词元转换为ID（vocab[tokens]会调用vocab的__getitem__方法）\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f562028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    \"\"\"\n",
    "    将所有样本填充到相同长度，转换为PyTorch张量\n",
    "    \n",
    "    参数：\n",
    "        examples: [(token_ids, pred_positions, mlm_labels, segments, is_next), ...]\n",
    "        max_len: 最大序列长度\n",
    "        vocab: 词表对象\n",
    "    \n",
    "    返回（7个列表，每个列表包含所有样本的对应数据）：\n",
    "        all_token_ids: 填充后的输入序列 [batch, max_len]\n",
    "        all_segments: 段落标识 [batch, max_len]\n",
    "        valid_lens: 有效长度（不含<pad>）[batch]\n",
    "        all_pred_positions: MLM预测位置 [batch, max_num_mlm_preds]\n",
    "        all_mlm_weights: MLM损失权重 [batch, max_num_mlm_preds]\n",
    "        all_mlm_labels: MLM预测标签 [batch, max_num_mlm_preds]\n",
    "        nsp_labels: NSP标签 [batch]\n",
    "    \"\"\"\n",
    "    # MLM预测的最大数量 = 序列长度 * 15%\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    \n",
    "    # 初始化存储列表\n",
    "    all_token_ids, all_segments, valid_lens = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    \n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # ===== 1. 填充token_ids到max_len =====\n",
    "        # 在序列末尾添加<pad>词元\n",
    "        padding_len = max_len - len(token_ids)\n",
    "        all_token_ids.append(torch.tensor(\n",
    "            token_ids + [vocab['<pad>']] * padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 2. 填充segments到max_len =====\n",
    "        # 填充部分的segment标识为0\n",
    "        all_segments.append(torch.tensor(\n",
    "            segments + [0] * padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 3. 记录有效长度（用于attention mask）=====\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        \n",
    "        # ===== 4. 填充pred_positions到max_num_mlm_preds =====\n",
    "        mlm_padding_len = max_num_mlm_preds - len(pred_positions)\n",
    "        all_pred_positions.append(torch.tensor(\n",
    "            pred_positions + [0] * mlm_padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 5. 设置mlm_weights（真实预测位置权重为1，填充位置权重为0）=====\n",
    "        # 这样在计算损失时，填充位置的损失会被过滤掉\n",
    "        all_mlm_weights.append(torch.tensor(\n",
    "            [1.0] * len(mlm_pred_label_ids) + [0.0] * mlm_padding_len,\n",
    "            dtype=torch.float32))\n",
    "        \n",
    "        # ===== 6. 填充mlm_labels =====\n",
    "        all_mlm_labels.append(torch.tensor(\n",
    "            mlm_pred_label_ids + [0] * mlm_padding_len, \n",
    "            dtype=torch.long))\n",
    "        \n",
    "        # ===== 7. NSP标签（True->1, False->0）=====\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    \n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bxc4skg4zb",
   "metadata": {},
   "source": [
    "## 第四步：填充数据到固定长度\n",
    "\n",
    "由于不同样本的序列长度不同，需要将它们填充到相同长度才能批量处理。\n",
    "\n",
    "### 需要填充的内容：\n",
    "| 数据 | 填充值 | 说明 |\n",
    "|------|--------|------|\n",
    "| token_ids | `<pad>` | 输入序列填充 |\n",
    "| segments | 0 | 段落标识填充 |\n",
    "| pred_positions | 0 | MLM预测位置填充 |\n",
    "| mlm_weights | 0.0 | 填充位置的损失权重为0 |\n",
    "| mlm_labels | 0 | MLM标签填充 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d92b28",
   "metadata": {},
   "outputs": [],
   "source": "class _WikiTextDataset(torch.utils.data.Dataset):\n    \"\"\"\n    WikiText-2 BERT预训练数据集\n    \n    数据处理流程：\n    1. 分词：将句子字符串转换为词元列表\n    2. 构建词表：基于所有句子构建词表\n    3. 生成NSP数据：为每个段落生成句子对\n    4. 生成MLM数据：对每个句子对进行遮蔽处理\n    5. 填充对齐：将所有样本填充到相同长度\n    \"\"\"\n    \n    def __init__(self, paragraphs, max_len):\n        \"\"\"\n        参数：\n            paragraphs: 段落列表，每个段落是句子字符串列表\n                       [[\"sentence1\", \"sentence2\"], [\"sentence3\", \"sentence4\"], ...]\n            max_len: 最大序列长度\n        \"\"\"\n        # ===== 步骤1: 分词 =====\n        # 将每个句子从字符串转换为词元列表\n        # 输入: [[\"hello world\", \"foo bar\"], ...]\n        # 输出: [[[\"hello\", \"world\"], [\"foo\", \"bar\"]], ...]\n        # 使用自定义tokenize函数（替代d2l.tokenize）\n        paragraphs = [tokenize(paragraph, token='word') \n                      for paragraph in paragraphs]\n        \n        # ===== 步骤2: 构建词表 =====\n        # 将所有句子展平成一个列表用于构建词表\n        sentences = [sentence for paragraph in paragraphs \n                     for sentence in paragraph]\n        # min_freq=5: 词频少于5的词会被替换为<unk>\n        # reserved_tokens: 预留的特殊词元\n        # 使用自定义Vocab类（替代d2l.Vocab）\n        self.vocab = Vocab(sentences, min_freq=5, reserved_tokens=[\n            '<pad>',   # 填充词元\n            '<mask>',  # MLM遮蔽词元\n            '<cls>',   # 句子开头词元\n            '<sep>'    # 句子分隔词元\n        ])\n        \n        # ===== 步骤3: 生成NSP数据 =====\n        # 为每个段落生成所有可能的句子对\n        examples = []\n        for paragraph in paragraphs:\n            examples.extend(_get_nsp_data_from_paragraph(\n                paragraph, paragraphs, self.vocab, max_len))\n        \n        # ===== 步骤4: 生成MLM数据 =====\n        # 对每个句子对进行遮蔽处理\n        # 输入: (tokens, segments, is_next)\n        # 输出: (token_ids, pred_positions, mlm_labels, segments, is_next)\n        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n                     + (segments, is_next))\n                    for tokens, segments, is_next in examples]\n        \n        # ===== 步骤5: 填充到固定长度 =====\n        (self.all_token_ids, self.all_segments, self.valid_lens,\n         self.all_pred_positions, self.all_mlm_weights,\n         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n            examples, max_len, self.vocab)\n\n    def __getitem__(self, idx):\n        \"\"\"返回第idx个样本的所有数据\"\"\"\n        return (self.all_token_ids[idx],      # 输入token IDs\n                self.all_segments[idx],        # 段落标识(0或1)\n                self.valid_lens[idx],          # 有效长度\n                self.all_pred_positions[idx],  # MLM预测位置\n                self.all_mlm_weights[idx],     # MLM损失权重\n                self.all_mlm_labels[idx],      # MLM预测标签\n                self.nsp_labels[idx])          # NSP标签(0或1)\n\n    def __len__(self):\n        \"\"\"返回数据集大小\"\"\"\n        return len(self.all_token_ids)"
  },
  {
   "cell_type": "markdown",
   "id": "f4pauaxp0u",
   "metadata": {},
   "source": [
    "## 第五步：创建PyTorch Dataset\n",
    "\n",
    "将所有数据处理步骤整合到一个Dataset类中，便于与DataLoader配合使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"\n",
    "    加载WikiText-2数据集用于BERT预训练\n",
    "    \n",
    "    参数：\n",
    "        batch_size: 批量大小\n",
    "        max_len: 最大序列长度（包含特殊词元）\n",
    "    \n",
    "    返回：\n",
    "        train_iter: 训练数据迭代器\n",
    "        vocab: 词表对象\n",
    "    \"\"\"\n",
    "    # 使用本地已解压的数据目录\n",
    "    data_dir = '../data/wikitext-2'\n",
    "    \n",
    "    # 读取并预处理数据\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    # 注意：在Jupyter notebook中需要设置num_workers=0\n",
    "    # 因为notebook中定义的类无法被pickle序列化用于多进程\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        train_set, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0  # notebook环境下必须为0\n",
    "    )\n",
    "    \n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unwu70myj2j",
   "metadata": {},
   "source": [
    "## 第六步：数据加载器\n",
    "\n",
    "将数据集包装成DataLoader，支持批量加载和多进程预取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "batch_size = 512  # 批量大小\n",
    "max_len = 64      # 最大序列长度\n",
    "\n",
    "# 加载数据\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "# 查看第一个batch的形状\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, \n",
    "     mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    \n",
    "    print(f\"tokens_X:        {tokens_X.shape}\")        # [512, 64] - 输入序列\n",
    "    print(f\"segments_X:      {segments_X.shape}\")      # [512, 64] - 段落标识\n",
    "    print(f\"valid_lens_x:    {valid_lens_x.shape}\")    # [512] - 有效长度\n",
    "    print(f\"pred_positions_X:{pred_positions_X.shape}\")# [512, 10] - MLM位置 (64*0.15≈10)\n",
    "    print(f\"mlm_weights_X:   {mlm_weights_X.shape}\")   # [512, 10] - MLM权重\n",
    "    print(f\"mlm_Y:           {mlm_Y.shape}\")           # [512, 10] - MLM标签\n",
    "    print(f\"nsp_y:           {nsp_y.shape}\")           # [512] - NSP标签\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ykphl6s1t",
   "metadata": {},
   "source": [
    "## 测试数据加载\n",
    "\n",
    "验证数据加载器是否正常工作，并查看各张量的形状。\n",
    "\n",
    "### 输出形状说明：\n",
    "| 张量 | 形状 | 说明 |\n",
    "|------|------|------|\n",
    "| tokens_X | [batch, max_len] | 输入token IDs |\n",
    "| segments_X | [batch, max_len] | 段落标识 |\n",
    "| valid_lens_x | [batch] | 每个样本的有效长度 |\n",
    "| pred_positions_X | [batch, max_mlm_preds] | MLM预测位置 |\n",
    "| mlm_weights_X | [batch, max_mlm_preds] | MLM损失权重 |\n",
    "| mlm_Y | [batch, max_mlm_preds] | MLM预测标签 |\n",
    "| nsp_y | [batch] | NSP标签 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看词表大小\n",
    "print(f\"词表大小: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de599add",
   "metadata": {},
   "outputs": [],
   "source": "# 该cell中的导入已经在第一个cell中完成，此处留空\n# 所有d2l依赖已替换为纯原生实现"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb7727",
   "metadata": {},
   "outputs": [],
   "source": "def transpose_qkv(X, num_heads):\n    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n    # 输入X的形状:(batch_size，查询或者\"键－值\"对的个数，num_hiddens)\n    # 输出X的形状:(batch_size，查询或者\"键－值\"对的个数，num_heads， num_hiddens/num_heads)\n    X= X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n    # 输出X的形状:(batch_size，num_heads，查询或者\"键－值\"对的个数, num_hiddens/num_heads)\n    X = X.permute(0, 2, 1, 3)\n    # 最终输出的形状:(batch_size*num_heads,查询或者\"键－值\"对的个数, num_hiddens/num_heads)\n    return X.reshape(-1, X.shape[2], X.shape[3])\n\n\ndef transpose_output(X, num_heads):\n    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n    X = X.permute(0, 2, 1, 3)\n    return X.reshape(X.shape[0], X.shape[1], -1)\n\n\nclass MultiHeaderAttention(nn.Module):\n    \"\"\"\n    多头注意力机制\n    \n    使用自定义的DotProductAttention（替代d2l.DotProductAttention）\n    \"\"\"\n    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False) -> None:\n        super().__init__()\n        self.num_heads = num_heads\n        # 使用自定义的DotProductAttention\n        self.attention = DotProductAttention(dropout)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n    \n    def forward(self, queries, keys, values, valid_lens):\n        # queries，keys，values的形状:\n        # (batch_size，查询或者\"键－值\"对的个数，num_hiddens)\n        # valid_lens　的形状:\n        # (batch_size，)或(batch_size，查询的个数)\n        # 经过变换后，输出的queries，keys，values　的形状:\n        # (batch_size*num_heads，查询或者\"键－值\"对的个数，num_hiddens/num_heads)\n        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n        values = transpose_qkv(self.W_v(values), self.num_heads)\n        \n        if valid_lens is not None:\n            # 将 valid_lens 重复 num_heads 次，因为每个注意力头都需要独立的 valid_lens\n            valid_lens = torch.repeat_interleave(\n                valid_lens, repeats=self.num_heads, dim=0\n            )\n        \n        output = self.attention(queries, keys, values, valid_lens)\n        \n        output_cat = transpose_output(output, num_heads=self.num_heads)\n        return self.W_o(output_cat)\n\n\nclass AddNorm(nn.Module):\n    \"\"\"残差连接 + 层归一化\"\"\"\n    def __init__(self, normalized_shape, dropout) -> None:\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(normalized_shape)\n    \n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n\n\nclass PositionWiseFFN(nn.Module):\n    \"\"\"基于位置的前馈网络\"\"\"\n    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs) -> None:\n        super().__init__()\n        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n    \n    def forward(self, X):\n        return self.dense2(self.relu(self.dense1(X)))\n\n\nclass EncoderBlock(nn.Module):\n    \"\"\"Transformer编码器块\"\"\"\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n                 dropout, use_bias=False, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n        # 使用自定义的MultiHeaderAttention\n        self.attention = MultiHeaderAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n            bias=use_bias)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(\n            ffn_num_input, ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n\n    def forward(self, X, valid_lens):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n        return self.addnorm2(Y, self.ffn(Y))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ac2d5",
   "metadata": {},
   "outputs": [],
   "source": "# get_tokens_and_segments函数已经在第一个cell中定义\n# 此处留空，避免重复定义"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fa6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BertEncoder\n",
    "    segment表示\"句子片段类型embedding\"。\n",
    "    在BERT中，输入通常是两段文本拼接，例如句子A和句子B。\n",
    "    segment用于区分不同的句子（例如A为0，B为1），以便模型能够知道某个token属于哪一部分。\n",
    "\n",
    "    输入是2，代表segment可以取两种类型（0或1）：0表示第一个句子片段，1表示第二个句子片段。\n",
    "    如果只输入单句任务，全部segment为0；如果是句子对任务，根据分割点设置为0和1。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embeding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # segment_embeding输入2，代表两种类型（句子1和句子2：0或1）\n",
    "        self.segment_embeding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                                                      ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 位置编码，shape=[1, max_len, num_hiddens]，可学习\n",
    "        self.pos_embeding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "    \n",
    "    def forward(self, token, segment, valid_lens):\n",
    "        \"\"\"\n",
    "        token: 词索引序列，[batch, seq_len]\n",
    "        segment: 句子片段类型，[batch, seq_len]，值为0或1\n",
    "        valid_lens: 有效长度\n",
    "        \"\"\"\n",
    "        # token embedding + segment embedding\n",
    "        X = self.token_embeding(token) + self.segment_embeding(segment)\n",
    "        # 加上可学习的位置编码\n",
    "        X = X + self.pos_embeding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e1d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768) -> None:\n",
    "        \"\"\"\n",
    "        Masked Language Model（MLM）模块。\n",
    "\n",
    "        参数说明：\n",
    "        vocab_size: 词表大小，输出类别数（即预测每个位置对应的词汇表token）。\n",
    "        num_hiddens: 隐藏层的维度。\n",
    "        num_inputs: 输入特征的维度，通常等于BERT编码器输出的隐藏单元数，默认768。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 构造一个MLP（多层感知机），输入num_inputs维，经过隐藏层后输出vocab_size维的预测。\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens), # 线性变换到隐藏层\n",
    "            nn.ReLU(),                          # 激活函数\n",
    "            nn.LayerNorm(num_hiddens),          # 层归一化\n",
    "            nn.Linear(num_hiddens, vocab_size)  # 映射到vocab_size，为softmax前的logits\n",
    "        )\n",
    "    \n",
    "    def forward(self, X, pred_positions):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "\n",
    "        参数：\n",
    "        X: 经过BERT encoder后的表示，形状为 [batch_size, seq_len, hidden_dim]，\n",
    "           表示每个token的上下文表示。\n",
    "        pred_positions: 要预测的masked位置索引，形状为 [batch_size, num_pred_positions]，\n",
    "                        每行是一个样本要预测的位置列表。\n",
    "\n",
    "        返回：\n",
    "        mlm_Y_hat: 每个被mask位置的预测结果，\n",
    "                   形状为 [batch_size, num_pred_positions, vocab_size]。\n",
    "        \"\"\"\n",
    "        # 1. 得到每个样本需要预测的token数量\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        # 2. 将pred_positions展平为一维，便于统一索引\n",
    "        pred_positions_flat = pred_positions.reshape(-1)  # 长度为batch_size * num_pred_positions\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        # 3. 构造一个batch索引。例如batch_size=2, num_pred_positions=3时，得到[0,0,0,1,1,1]\n",
    "        batch_idx = torch.arange(0, batch_size).repeat_interleave(num_pred_positions)\n",
    "        # 这样(X[batch_idx, pred_positions_flat])就取出所有需要mask的token的表示\n",
    "\n",
    "        # 4. 按指定位置收集得到被mask位置的上下文表示，形状为 [batch_size * num_pred_positions, hidden_dim]\n",
    "        masked_X = X[batch_idx, pred_positions_flat]\n",
    "        # 5. 恢复成 [batch_size, num_pred_positions, hidden_dim] 的形式\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "\n",
    "        # 6. 通过MLP变换，每个位置最终输出vocab_size维，对应softmax前的logits\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "\n",
    "        # 7. 输出，形状为 [batch_size, num_pred_positions, vocab_size]\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8273bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, num_inputs) -> None:\n",
    "        super().__init__()\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03186180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,\n",
    "                 nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BertEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f889ef",
   "metadata": {},
   "outputs": [],
   "source": "# 创建BERT模型\n# max_len设置为64（与数据集的max_len一致）\nnet = BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],\n                ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,\n                num_layers=2, dropout=0.2, key_size=128, query_size=128,\n                value_size=128, hid_in_features=128, mlm_in_features=128,\n                nsp_in_features=128, max_len=64)\n\n# 使用自定义的try_all_gpus函数（替代d2l.try_all_gpus）\ndevices = try_all_gpus()\nloss = nn.CrossEntropyLoss()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78650ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # 前向传播\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # 计算遮蔽语言模型损失\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # 计算下一句子预测任务的损失\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb9028",
   "metadata": {},
   "outputs": [],
   "source": "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n    \"\"\"\n    训练BERT模型\n    \n    使用自定义的Timer、Animator、Accumulator（替代d2l版本）\n    \"\"\"\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n    step = 0\n    timer = Timer()  # 使用自定义Timer\n    animator = Animator(xlabel='step', ylabel='loss',\n                        xlim=[1, num_steps], legend=['mlm', 'nsp'])  # 使用自定义Animator\n    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数\n    metric = Accumulator(4)  # 使用自定义Accumulator\n    num_steps_reached = False\n    while step < num_steps and not num_steps_reached:\n        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n            tokens_X = tokens_X.to(devices[0])\n            segments_X = segments_X.to(devices[0])\n            valid_lens_x = valid_lens_x.to(devices[0])\n            pred_positions_X = pred_positions_X.to(devices[0])\n            mlm_weights_X = mlm_weights_X.to(devices[0])\n            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n            trainer.zero_grad()\n            timer.start()\n            mlm_l, nsp_l, l = _get_batch_loss_bert(\n                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n            l.backward()\n            trainer.step()\n            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n            timer.stop()\n            animator.add(step + 1,\n                         (metric[0] / metric[3], metric[1] / metric[3]))\n            step += 1\n            if step == num_steps:\n                num_steps_reached = True\n                break\n\n    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n          f'NSP loss {metric[1] / metric[3]:.3f}')\n    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n          f'{str(devices)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert(train_iter, net, loss, len(vocab), devices, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1815982",
   "metadata": {},
   "outputs": [],
   "source": "def get_bert_encoding(net, tokens_a, tokens_b=None):\n    \"\"\"\n    获取BERT编码\n    \n    使用自定义的get_tokens_and_segments函数（替代d2l版本）\n    \"\"\"\n    tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n    encoded_X, _, _ = net(token_ids, segments, valid_len)\n    return encoded_X"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面，我们以\"a crane is flying\"这个句子作为输入，获取其BERT编码\n",
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "\n",
    "# encoded_text的形状为(1, 7, 隐藏层维数)，7表示句子的tokens数量（包括特殊token，如<cls>、<sep>等）\n",
    "print(\"encoded_text.shape:\", encoded_text.shape)\n",
    "\n",
    "# [CLS]位置的输出表示整个句子的语义，用于句子级任务\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "print(\"encoded_text_cls.shape:\", encoded_text_cls.shape)\n",
    "\n",
    "# \"crane\"单词在tokens中的位置是2，获取其BERT编码\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "print(\"encoded_text_crane向量的前3维:\", encoded_text_crane[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\n",
    "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
    "# 词元：'<cls>','a','crane','driver','came','<sep>','he','just',\n",
    "# 'left','<sep>'\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}