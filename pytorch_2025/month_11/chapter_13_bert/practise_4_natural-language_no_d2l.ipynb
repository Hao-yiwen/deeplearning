{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5044dcbc",
   "metadata": {},
   "source": [
    "# 自然语言推断与数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8818f8",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport os\nimport re\nimport zipfile\nimport hashlib\nimport requests\nimport collections\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport time\n\n# ==================== 替代d2l的工具函数 ====================\n\n# 数据目录\nDATA_HUB = {}\nDATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n\ndef download(name, cache_dir=os.path.join('..', 'data')):\n    \"\"\"下载一个DATA_HUB中的文件，返回本地文件路径\"\"\"\n    assert name in DATA_HUB, f\"{name} 不存在于 DATA_HUB 中\"\n    url, sha1_hash = DATA_HUB[name]\n    os.makedirs(cache_dir, exist_ok=True)\n    fname = os.path.join(cache_dir, url.split('/')[-1])\n    if os.path.exists(fname):\n        sha1 = hashlib.sha1()\n        with open(fname, 'rb') as f:\n            while True:\n                data = f.read(1048576)\n                if not data:\n                    break\n                sha1.update(data)\n        if sha1.hexdigest() == sha1_hash:\n            return fname  # 命中缓存\n    print(f'正在从 {url} 下载 {fname}...')\n    r = requests.get(url, stream=True, verify=True)\n    with open(fname, 'wb') as f:\n        f.write(r.content)\n    return fname\n\ndef download_extract(name, folder=None):\n    \"\"\"下载并解压zip/tar文件\"\"\"\n    fname = download(name)\n    base_dir = os.path.dirname(fname)\n    data_dir, ext = os.path.splitext(fname)\n    if ext == '.zip':\n        fp = zipfile.ZipFile(fname, 'r')\n    else:\n        raise NotImplementedError(\"只支持zip格式\")\n    fp.extractall(base_dir)\n    return os.path.join(base_dir, folder) if folder else data_dir\n\ndef tokenize(lines, token='word'):\n    \"\"\"将文本行列表分词\"\"\"\n    if token == 'word':\n        return [line.split() for line in lines]\n    elif token == 'char':\n        return [list(line) for line in lines]\n    else:\n        raise ValueError(f\"未知token类型: {token}\")\n\nclass Vocab:\n    \"\"\"文本词表\"\"\"\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # 按出现频率排序\n        counter = count_corpus(tokens)\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n        # 未知词元的索引为0\n        self.idx_to_token = ['<unk>'] + reserved_tokens\n        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if freq < min_freq:\n                break\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\n    @property\n    def unk(self):\n        return 0\n\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):\n    \"\"\"统计词元频率\"\"\"\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n\ndef truncate_pad(line, num_steps, padding_token):\n    \"\"\"截断或填充文本序列\"\"\"\n    if len(line) > num_steps:\n        return line[:num_steps]\n    return line + [padding_token] * (num_steps - len(line))\n\ndef try_all_gpus():\n    \"\"\"返回所有可用的GPU，如果没有GPU则返回[cpu()]\"\"\"\n    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n    return devices if devices else [torch.device('cpu')]\n\ndef try_gpu(i=0):\n    \"\"\"返回gpu(i)，如果不存在则返回cpu()\"\"\"\n    if torch.cuda.device_count() >= i + 1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')\n\ndef get_dataloader_workers():\n    \"\"\"获取dataloader的worker数量\"\"\"\n    return 0  # 在macOS上使用0更稳定\n\ndef get_tokens_and_segments(tokens_a, tokens_b=None):\n    \"\"\"获取BERT格式的输入tokens和segments\"\"\"\n    tokens = ['<cls>'] + tokens_a + ['<sep>']\n    segments = [0] * (len(tokens_a) + 2)\n    if tokens_b is not None:\n        tokens = tokens + tokens_b + ['<sep>']\n        segments = segments + [1] * (len(tokens_b) + 1)\n    return tokens, segments\n\nclass Accumulator:\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\nclass Timer:\n    \"\"\"记录多次运行时间\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start()\n\n    def start(self):\n        self.tik = time.time()\n\n    def stop(self):\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n\n    def avg(self):\n        return sum(self.times) / len(self.times)\n\n    def sum(self):\n        return sum(self.times)\n\n    def cumsum(self):\n        return list(accumulate(self.times))\n\ndef accuracy(y_hat, y):\n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())\n\ndef evaluate_accuracy_gpu(net, data_iter, device=None):\n    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n    if isinstance(net, nn.Module):\n        net.eval()\n        if not device:\n            device = next(iter(net.parameters())).device\n    metric = Accumulator(2)\n    with torch.no_grad():\n        for X, y in data_iter:\n            if isinstance(X, (list, tuple)):\n                # BERT微调所需\n                X = [x.to(device) for x in X]\n            else:\n                X = X.to(device)\n            y = y.to(device)\n            metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\ndef train_batch_ch13(net, X, y, loss, trainer, device):\n    \"\"\"进行小批量训练\"\"\"\n    if isinstance(X, (list, tuple)):\n        X = [x.to(device) for x in X]\n    else:\n        X = X.to(device)\n    y = y.to(device)\n    net.train()\n    trainer.zero_grad()\n    pred = net(X)\n    l = loss(pred, y)\n    l.sum().backward()\n    trainer.step()\n    train_loss_sum = l.sum()\n    train_acc_sum = accuracy(pred, y)\n    return train_loss_sum, train_acc_sum\n\ndef train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices):\n    \"\"\"进行模型训练（支持单设备和多GPU）\"\"\"\n    timer, num_batches = Timer(), len(train_iter)\n    device = devices[0]\n    \n    # 只在有多个CUDA设备时使用DataParallel\n    if len(devices) > 1 and devices[0].type == 'cuda':\n        net = nn.DataParallel(net, device_ids=devices)\n    net = net.to(device)\n    \n    for epoch in range(num_epochs):\n        # 4个维度：训练损失之和，训练准确率之和，样本数，标签数\n        metric = Accumulator(4)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = train_batch_ch13(net, features, labels, loss, trainer, device)\n            metric.add(l, acc, labels.shape[0], labels.numel())\n            timer.stop()\n            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n                print(f'epoch {epoch + 1}, batch {i + 1}/{num_batches}, '\n                      f'loss {metric[0] / metric[2]:.3f}, '\n                      f'train acc {metric[1] / metric[3]:.3f}')\n        test_acc = evaluate_accuracy_gpu(net, test_iter, device)\n        print(f'epoch {epoch + 1}, test acc {test_acc:.3f}')\n    print(f'loss {metric[0] / metric[2]:.3f}, train acc {metric[1] / metric[3]:.3f}, '\n          f'test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on {str(devices)}')"
  },
  {
   "cell_type": "markdown",
   "id": "ba2e8110",
   "metadata": {},
   "source": [
    "## SNLI 数据集文件格式说明\n",
    "\n",
    "SNLI 数据集提供了两种格式的文件：\n",
    "\n",
    "### 1. `.txt` 文件（制表符分隔格式，TSV）\n",
    "- **格式**：每行一个样本，字段用制表符（`\\t`）分隔\n",
    "- **表头**：第一行包含字段名\n",
    "- **字段**：\n",
    "  - `gold_label`: 标签（entailment/contradiction/neutral）\n",
    "  - `sentence1`: 前提句子（premise）\n",
    "  - `sentence2`: 假设句子（hypothesis）\n",
    "  - `sentence1_binary_parse`: 句法树（二进制格式）\n",
    "  - `sentence2_binary_parse`: 句法树（二进制格式）\n",
    "  - `sentence1_parse`: 句法树（Penn Treebank格式）\n",
    "  - `sentence2_parse`: 句法树（Penn Treebank格式）\n",
    "  - `captionID`, `pairID`: 唯一标识符\n",
    "  - `label1-5`: 5个标注者的标签\n",
    "- **优点**：易于用 `split('\\t')` 解析，适合简单脚本处理\n",
    "- **示例**：\n",
    "  ```\n",
    "  gold_label\tsentence1\tsentence2\t...\n",
    "  neutral\tTwo women are embracing...\tThe sisters are hugging...\t...\n",
    "  ```\n",
    "\n",
    "### 2. `.jsonl` 文件（JSON Lines 格式）\n",
    "- **格式**：每行一个 JSON 对象\n",
    "- **字段**：与 `.txt` 文件相同，但以 JSON 格式存储\n",
    "- **优点**：\n",
    "  - 结构化数据，易于解析嵌套字段\n",
    "  - 支持复杂数据结构\n",
    "  - 更适合程序化处理\n",
    "- **示例**：\n",
    "  ```json\n",
    "  {\"gold_label\": \"neutral\", \"sentence1\": \"Two women are embracing...\", \"sentence2\": \"The sisters are hugging...\", ...}\n",
    "  ```\n",
    "\n",
    "### 关系总结\n",
    "- **数据内容完全相同**：两种文件包含相同的数据，只是格式不同\n",
    "- **字段对应关系**：`.txt` 的列对应 `.jsonl` 的 JSON 键\n",
    "- **使用建议**：\n",
    "  - 简单文本处理：使用 `.txt` 文件（当前代码使用）\n",
    "  - 需要结构化数据：使用 `.jsonl` 文件（用 `json.loads()` 解析）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7525ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册SNLI数据集\n",
    "DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = download_extract('SNLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6871af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"\n",
    "    该函数用于读取SNLI（Stanford Natural Language Inference）数据集，并将其解析为前提（premise）、假设（hypothesis）和标签（label）。\n",
    "    \n",
    "    参数:\n",
    "        data_dir (str): 数据集所在的目录\n",
    "        is_train (bool): 是否读取训练集数据。如果为True，则读取训练集；否则读取测试集。\n",
    "\n",
    "    返回:\n",
    "        premises (list of str): 前提句子的列表\n",
    "        hypotheses (list of str): 假设句子的列表\n",
    "        labels (list of int): 标签的列表，0代表entailment，1代表contradiction，2代表neutral\n",
    "    \"\"\"\n",
    "    def extract_text(s):\n",
    "        # 此辅助函数用于对原始文本进行清洗。它将删除括号，\n",
    "        # 并将连续的多个空格替换为一个空格，最后去除首尾空格。\n",
    "        s = re.sub('\\\\(', '', s)        # 去除左括号\n",
    "        s = re.sub('\\\\)', '', s)        # 去除右括号\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)   # 多个空格替换为一个空格\n",
    "        return s.strip()                # 去除首尾空格\n",
    "\n",
    "    # 定义标签与数字的对应关系:\n",
    "    # entailment-蕴涵(0), contradiction-矛盾(1), neutral-中立(2)\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "\n",
    "    # 根据is_train标志选择加载训练集还是测试集\n",
    "    file_name = os.path.join(\n",
    "        data_dir, \n",
    "        'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt'\n",
    "    )\n",
    "\n",
    "    # 打开并读取数据文件，跳过第一行表头\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 依次抽取前提、假设和标签\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3d873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is training his horse for a competition .\n",
      "标签： 2\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is at a diner , ordering an omelette .\n",
      "标签： 1\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is outdoors , on a horse .\n",
      "标签： 0\n"
     ]
    }
   ],
   "source": [
    "train_data = read_snli(data_dir, True)\n",
    "for x,y,z in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('前提：', x)\n",
    "    print('假设：', y)\n",
    "    print('标签：', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b829634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各类别样本数（[entailment, contradiction, neutral]）: [183416, 183187, 182764]\n",
      "各类别样本数（[entailment, contradiction, neutral]）: [3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "test_data = read_snli(data_dir, False)\n",
    "# train_data 和 test_data 都是 read_snli 的返回结果\n",
    "# 它们的格式都是 (premises, hypotheses, labels)，即长度为3的元组：\n",
    "# [0] 是前提句子列表， [1] 是假设句子列表， [2] 是标签列表\n",
    "# 所以 data[2] 就是标签列表\n",
    "for data in [train_data, test_data]:\n",
    "    counts = [data[2].count(i) for i in range(3)]\n",
    "    print('各类别样本数（[entailment, contradiction, neutral]）:', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c303e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： ['前提A1', '前提A2']\n",
      "假设： ['假设A1', '假设A2']\n",
      "标签： [0, 1]\n",
      "\n",
      "前提： ['前提B1']\n",
      "假设： ['假设B1']\n",
      "标签： [2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 最简单例子：假设有 train_data 和 test_data 都是 (前提, 假设, 标签) 构成的元组\n",
    "ex1 = ([\"前提A1\", \"前提A2\"], [\"假设A1\", \"假设A2\"], [0, 1])\n",
    "ex2 = ([\"前提B1\"], [\"假设B1\"], [2])\n",
    "for data in [ex1,ex2]:\n",
    "    print(\"前提：\", data[0])\n",
    "    print(\"假设：\", data[1])\n",
    "    print(\"标签：\", data[2])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, num_steps, vocab=None) -> None:\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            # 这里创建一个词表（vocab），包含前提句子和假设句子的所有分词。\n",
    "            # min_freq=5表示只保留至少出现5次的词，\n",
    "            # reserved_tokens参数则额外添加了'<pad>'（填充符号）作为特殊标记。\n",
    "            self.vocab = Vocab(all_premise_tokens + all_hypothesis_tokens,\n",
    "                    min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "    \n",
    "    def _pad(self, lines):\n",
    "        # 解释：这个方法将每个句子（line）分词后转为词表（vocab）对应的索引序列，\n",
    "        # 然后通过 truncate_pad 函数截断或填充到固定长度 num_steps，\n",
    "        # 不足的用 <pad> 对应的索引补齐。\n",
    "        # 最终所有句子转为形状一致的张量返回，便于批量输入神经网络。\n",
    "        return torch.tensor([truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>']\n",
    "        ) for line in lines])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx], self.labels[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6568c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    num_workers = get_dataloader_workers()\n",
    "    data_dir = download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d8685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1016424",
   "metadata": {},
   "source": [
    "## 数据加载器返回格式说明\n",
    "\n",
    "`SNLIDataset` 的 `__getitem__` 方法返回3个元素：\n",
    "```python\n",
    "return (self.premises[idx], self.hypotheses[idx], self.labels[idx])\n",
    "```\n",
    "\n",
    "因此，`DataLoader` 批量加载时也会返回3个元素的元组：\n",
    "- **premises**: 前提句子的批次，形状 `[batch_size, num_steps]`\n",
    "- **hypotheses**: 假设句子的批次，形状 `[batch_size, num_steps]`\n",
    "- **labels**: 标签的批次，形状 `[batch_size]`\n",
    "\n",
    "**正确的解包方式**：\n",
    "```python\n",
    "for premises, hypotheses, labels in train_iter:\n",
    "    # premises: [batch_size, num_steps]\n",
    "    # hypotheses: [batch_size, num_steps]\n",
    "    # labels: [batch_size]\n",
    "```\n",
    "\n",
    "**错误的解包方式**（会导致 ValueError）：\n",
    "```python\n",
    "for X, Y in train_iter:  # ❌ 期望2个值，但实际有3个\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a3184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premises shape:   torch.Size([128, 50])\n",
      "hypotheses shape: torch.Size([128, 50])\n",
      "labels shape:     torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# SNLIDataset 返回3个元素：(premises, hypotheses, labels)\n",
    "# DataLoader 批量加载时，返回的也是3个元素的元组：\n",
    "# - 第1个元素：premises 批次 [batch_size, num_steps]\n",
    "# - 第2个元素：hypotheses 批次 [batch_size, num_steps]  \n",
    "# - 第3个元素：labels 批次 [batch_size]\n",
    "for premises, hypotheses, labels in train_iter:\n",
    "    print(f\"premises shape:   {premises.shape}\")    # [batch_size, num_steps]\n",
    "    print(f\"hypotheses shape: {hypotheses.shape}\")  # [batch_size, num_steps]\n",
    "    print(f\"labels shape:     {labels.shape}\")      # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4916fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad12fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册预训练BERT模型\n",
    "DATA_HUB['bert.base'] = (DATA_URL + 'bert.base.torch.zip',\n",
    "                         '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "DATA_HUB['bert.small'] = (DATA_URL + 'bert.small.torch.zip',\n",
    "                          'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "q4tbwaab3zf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为多头注意力变换形状\"\"\"\n",
    "    # X: (batch_size, num_queries/num_keys, num_hiddens)\n",
    "    # -> (batch_size, num_queries/num_keys, num_heads, num_hiddens/num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    # -> (batch_size, num_heads, num_queries/num_keys, num_hiddens/num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # -> (batch_size * num_heads, num_queries/num_keys, num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv的操作\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 在最后的轴上，被掩蔽的元素使用一个非常大的负值替换，使softmax输出为0\n",
    "        X = X.reshape(-1, shape[-1])\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_lens[:, None]\n",
    "        X[~mask] = -1e6\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8c2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80dc038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BertEncoder\n",
    "    segment表示\"句子片段类型embedding\"。\n",
    "    在BERT中，输入通常是两段文本拼接，例如句子A和句子B。\n",
    "    segment用于区分不同的句子（例如A为0，B为1），以便模型能够知道某个token属于哪一部分。\n",
    "\n",
    "    输入是2，代表segment可以取两种类型（0或1）：0表示第一个句子片段，1表示第二个句子片段。\n",
    "    如果只输入单句任务，全部segment为0；如果是句子对任务，根据分割点设置为0和1。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768) -> None:\n",
    "        super().__init__()\n",
    "        # 修复拼写：embeding -> embedding（与预训练模型保持一致）\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # segment_embedding输入2，代表两种类型（句子1和句子2：0或1）\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                                                      ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 位置编码，shape=[1, max_len, num_hiddens]，可学习\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "    \n",
    "    def forward(self, token, segment, valid_lens):\n",
    "        \"\"\"\n",
    "        token: 词索引序列，[batch, seq_len]\n",
    "        segment: 句子片段类型，[batch, seq_len]，值为0或1\n",
    "        valid_lens: 有效长度\n",
    "        \"\"\"\n",
    "        # token embedding + segment embedding\n",
    "        X = self.token_embedding(token) + self.segment_embedding(segment)\n",
    "        # 加上可学习的位置编码\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08949398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768) -> None:\n",
    "        \"\"\"\n",
    "        Masked Language Model（MLM）模块。\n",
    "\n",
    "        参数说明：\n",
    "        vocab_size: 词表大小，输出类别数（即预测每个位置对应的词汇表token）。\n",
    "        num_hiddens: 隐藏层的维度。\n",
    "        num_inputs: 输入特征的维度，通常等于BERT编码器输出的隐藏单元数，默认768。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 构造一个MLP（多层感知机），输入num_inputs维，经过隐藏层后输出vocab_size维的预测。\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens), # 线性变换到隐藏层\n",
    "            nn.ReLU(),                          # 激活函数\n",
    "            nn.LayerNorm(num_hiddens),          # 层归一化\n",
    "            nn.Linear(num_hiddens, vocab_size)  # 映射到vocab_size，为softmax前的logits\n",
    "        )\n",
    "    \n",
    "    def forward(self, X, pred_positions):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "\n",
    "        参数：\n",
    "        X: 经过BERT encoder后的表示，形状为 [batch_size, seq_len, hidden_dim]，\n",
    "           表示每个token的上下文表示。\n",
    "        pred_positions: 要预测的masked位置索引，形状为 [batch_size, num_pred_positions]，\n",
    "                        每行是一个样本要预测的位置列表。\n",
    "\n",
    "        返回：\n",
    "        mlm_Y_hat: 每个被mask位置的预测结果，\n",
    "                   形状为 [batch_size, num_pred_positions, vocab_size]。\n",
    "        \"\"\"\n",
    "        # 1. 得到每个样本需要预测的token数量\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        # 2. 将pred_positions展平为一维，便于统一索引\n",
    "        pred_positions_flat = pred_positions.reshape(-1)  # 长度为batch_size * num_pred_positions\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        # 3. 构造一个batch索引。例如batch_size=2, num_pred_positions=3时，得到[0,0,0,1,1,1]\n",
    "        batch_idx = torch.arange(0, batch_size).repeat_interleave(num_pred_positions)\n",
    "        # 这样(X[batch_idx, pred_positions_flat])就取出所有需要mask的token的表示\n",
    "\n",
    "        # 4. 按指定位置收集得到被mask位置的上下文表示，形状为 [batch_size * num_pred_positions, hidden_dim]\n",
    "        masked_X = X[batch_idx, pred_positions_flat]\n",
    "        # 5. 恢复成 [batch_size, num_pred_positions, hidden_dim] 的形式\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "\n",
    "        # 6. 通过MLP变换，每个位置最终输出vocab_size维，对应softmax前的logits\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "\n",
    "        # 7. 输出，形状为 [batch_size, num_pred_positions, vocab_size]\n",
    "        return mlm_Y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87aecc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, num_inputs) -> None:\n",
    "        super().__init__()\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.output(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58b604bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,\n",
    "                 nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BertEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是\"<cls>\"标记的索引\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fd7bc",
   "metadata": {},
   "outputs": [],
   "source": "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n                          num_heads, num_layers, dropout, max_len, devices):\n    data_dir = download_extract(pretrained_model)\n    # 定义空词表以加载预定义词表\n    vocab = Vocab()\n    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n    vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n    \n    # 根据模型类型设置参数\n    if 'small' in pretrained_model:\n        # bert.small: 2层, 256维, 4头, FFN=512\n        hidden_size, num_layers_model, num_heads_model, ffn_hiddens = 256, 2, 4, 512\n    else:\n        # bert.base: 12层, 768维, 12头, FFN=3072 (真正的BERT-base规格)\n        hidden_size, num_layers_model, num_heads_model, ffn_hiddens = 768, 12, 12, 3072\n    \n    bert = BERTModel(len(vocab), hidden_size, norm_shape=[hidden_size],\n                     ffn_num_input=hidden_size, ffn_num_hiddens=ffn_hiddens,\n                     num_heads=num_heads_model, num_layers=num_layers_model, dropout=dropout,\n                     max_len=max_len, key_size=hidden_size, query_size=hidden_size,\n                     value_size=hidden_size, hid_in_features=hidden_size,\n                     mlm_in_features=hidden_size, nsp_in_features=hidden_size)\n    # 加载预训练BERT参数\n    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params'),\n                                    map_location='cpu'))\n    return bert, vocab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert.base', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    num_layers=2, dropout=0.1, max_len=512, devices=devices)\n",
    "print(f\"加载模型: bert.base, 词表大小: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        # 对前提和假设句子进行小写处理和分词\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17131d0",
   "metadata": {},
   "outputs": [],
   "source": "# bert.base 模型较大，需要减小 batch_size 避免显存溢出\n# bert.small: batch_size=512 可以\n# bert.base: batch_size=32-64 比较合适\nbatch_size, max_len, num_workers = 32, 128, get_dataloader_workers()\ndata_dir = download_extract('SNLI')\ntrain_set = SNLIBERTDataset(read_snli(data_dir, True), max_len, vocab)\ntest_set = SNLIBERTDataset(read_snli(data_dir, False), max_len, vocab)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n                                   num_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(test_set, batch_size,\n                                  num_workers=num_workers)\nprint(f\"batch_size: {batch_size}, 训练批次数: {len(train_iter)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12d66c",
   "metadata": {},
   "outputs": [],
   "source": "class BERTClassifier(nn.Module):\n    def __init__(self, bert):\n        super().__init__()\n        self.encoder = bert.encoder\n        self.hidden = bert.hidden\n        # 动态获取隐藏层维度（从bert.hidden的输出维度）\n        hidden_size = bert.hidden[0].out_features\n        self.output = nn.Linear(hidden_size, 3)\n    \n    def forward(self, inputs):\n        tokens_X, segment_X, valid_lens_x = inputs\n        encoded_X = self.encoder(tokens_X, segment_X, valid_lens_x)\n        return self.output(self.hidden(encoded_X[:, 0, :]))"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e912b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.0004, 5\n",
    "trainer = torch.optim.AdamW(net.parameters(), lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jerihus8guh",
   "metadata": {},
   "source": [
    "## 预测函数\n",
    "\n",
    "训练完成后，我们可以使用模型对新的句子对进行自然语言推断预测。\n",
    "\n",
    "- **entailment (蕴涵)**: 前提能推出假设\n",
    "- **contradiction (矛盾)**: 前提与假设矛盾\n",
    "- **neutral (中立)**: 前提与假设无明确关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6s40ux2znq",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_snli(net, vocab, premise, hypothesis, max_len=128, device=None):\n",
    "    \"\"\"\n",
    "    对单个句子对进行自然语言推断预测\n",
    "    \n",
    "    参数:\n",
    "        net: 训练好的BERT分类器模型\n",
    "        vocab: 词表\n",
    "        premise: 前提句子（字符串）\n",
    "        hypothesis: 假设句子（字符串）\n",
    "        max_len: 最大序列长度\n",
    "        device: 计算设备\n",
    "    \n",
    "    返回:\n",
    "        预测的标签名称和概率\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(iter(net.parameters())).device\n",
    "    \n",
    "    # 标签映射\n",
    "    label_names = ['entailment', 'contradiction', 'neutral']\n",
    "    \n",
    "    # 1. 分词（转小写并按空格分割）\n",
    "    p_tokens = premise.lower().split()\n",
    "    h_tokens = hypothesis.lower().split()\n",
    "    \n",
    "    # 2. 截断过长的句子对\n",
    "    while len(p_tokens) + len(h_tokens) > max_len - 3:\n",
    "        if len(p_tokens) > len(h_tokens):\n",
    "            p_tokens.pop()\n",
    "        else:\n",
    "            h_tokens.pop()\n",
    "    \n",
    "    # 3. 构建BERT格式的输入: [CLS] premise [SEP] hypothesis [SEP]\n",
    "    tokens, segments = get_tokens_and_segments(p_tokens, h_tokens)\n",
    "    \n",
    "    # 4. 转换为token ids并填充\n",
    "    token_ids = vocab[tokens] + [vocab['<pad>']] * (max_len - len(tokens))\n",
    "    segments = segments + [0] * (max_len - len(segments))\n",
    "    valid_len = len(tokens)\n",
    "    \n",
    "    # 5. 转为张量并添加batch维度\n",
    "    token_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "    segments = torch.tensor([segments], dtype=torch.long, device=device)\n",
    "    valid_lens = torch.tensor([valid_len], device=device)\n",
    "    \n",
    "    # 6. 模型推断\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # 注意：BERTClassifier的forward接收的是元组 (tokens, segments, valid_lens)\n",
    "        outputs = net((token_ids, segments, valid_lens))\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        pred_label = outputs.argmax(dim=1).item()\n",
    "    \n",
    "    return label_names[pred_label], probs[0].cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2mqh58h5dqz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试预测函数\n",
    "# 注意：训练后net被包装成DataParallel，需要用net.module获取原始模型\n",
    "# 如果只有单GPU或CPU，可以直接使用net\n",
    "\n",
    "# 获取实际的模型（处理DataParallel包装）\n",
    "if isinstance(net, nn.DataParallel):\n",
    "    model = net.module\n",
    "else:\n",
    "    model = net\n",
    "\n",
    "device = devices[0]\n",
    "model = model.to(device)\n",
    "\n",
    "# 示例1: 蕴涵关系 (entailment)\n",
    "premise1 = \"A person is riding a horse in the park.\"\n",
    "hypothesis1 = \"Someone is outdoors with an animal.\"\n",
    "label1, probs1 = predict_snli(model, vocab, premise1, hypothesis1, device=device)\n",
    "print(f\"前提: {premise1}\")\n",
    "print(f\"假设: {hypothesis1}\")\n",
    "print(f\"预测: {label1}\")\n",
    "print(f\"概率: entailment={probs1[0]:.3f}, contradiction={probs1[1]:.3f}, neutral={probs1[2]:.3f}\")\n",
    "print()\n",
    "\n",
    "# 示例2: 矛盾关系 (contradiction)\n",
    "premise2 = \"A man is sleeping on the couch.\"\n",
    "hypothesis2 = \"The man is running in the marathon.\"\n",
    "label2, probs2 = predict_snli(model, vocab, premise2, hypothesis2, device=device)\n",
    "print(f\"前提: {premise2}\")\n",
    "print(f\"假设: {hypothesis2}\")\n",
    "print(f\"预测: {label2}\")\n",
    "print(f\"概率: entailment={probs2[0]:.3f}, contradiction={probs2[1]:.3f}, neutral={probs2[2]:.3f}\")\n",
    "print()\n",
    "\n",
    "# 示例3: 中立关系 (neutral)\n",
    "premise3 = \"A woman is playing the piano.\"\n",
    "hypothesis3 = \"The woman is a professional musician.\"\n",
    "label3, probs3 = predict_snli(model, vocab, premise3, hypothesis3, device=device)\n",
    "print(f\"前提: {premise3}\")\n",
    "print(f\"假设: {hypothesis3}\")\n",
    "print(f\"预测: {label3}\")\n",
    "print(f\"概率: entailment={probs3[0]:.3f}, contradiction={probs3[1]:.3f}, neutral={probs3[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kqgj5qlks4",
   "metadata": {},
   "source": [
    "## 批量预测\n",
    "\n",
    "如果需要对多个句子对进行预测，可以使用批量预测函数来提高效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vv9ksgdw5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_snli_batch(net, vocab, premises, hypotheses, max_len=128, device=None):\n",
    "    \"\"\"\n",
    "    批量预测多个句子对\n",
    "    \n",
    "    参数:\n",
    "        net: 训练好的BERT分类器模型\n",
    "        vocab: 词表\n",
    "        premises: 前提句子列表\n",
    "        hypotheses: 假设句子列表\n",
    "        max_len: 最大序列长度\n",
    "        device: 计算设备\n",
    "    \n",
    "    返回:\n",
    "        预测标签列表和概率列表\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(iter(net.parameters())).device\n",
    "    \n",
    "    label_names = ['entailment', 'contradiction', 'neutral']\n",
    "    \n",
    "    all_token_ids = []\n",
    "    all_segments = []\n",
    "    all_valid_lens = []\n",
    "    \n",
    "    for premise, hypothesis in zip(premises, hypotheses):\n",
    "        p_tokens = premise.lower().split()\n",
    "        h_tokens = hypothesis.lower().split()\n",
    "        \n",
    "        while len(p_tokens) + len(h_tokens) > max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "        \n",
    "        tokens, segments = get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = vocab[tokens] + [vocab['<pad>']] * (max_len - len(tokens))\n",
    "        segments = segments + [0] * (max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        \n",
    "        all_token_ids.append(token_ids)\n",
    "        all_segments.append(segments)\n",
    "        all_valid_lens.append(valid_len)\n",
    "    \n",
    "    # 转为张量\n",
    "    token_ids = torch.tensor(all_token_ids, dtype=torch.long, device=device)\n",
    "    segments = torch.tensor(all_segments, dtype=torch.long, device=device)\n",
    "    valid_lens = torch.tensor(all_valid_lens, device=device)\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = net((token_ids, segments, valid_lens))\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        pred_labels = outputs.argmax(dim=1).tolist()\n",
    "    \n",
    "    return [label_names[l] for l in pred_labels], probs.cpu().tolist()\n",
    "\n",
    "# 批量预测测试\n",
    "test_premises = [\n",
    "    \"Two dogs are running in the field.\",\n",
    "    \"A child is reading a book.\",\n",
    "    \"The sun is shining brightly.\"\n",
    "]\n",
    "test_hypotheses = [\n",
    "    \"Animals are playing outside.\",\n",
    "    \"The child is sleeping.\",\n",
    "    \"It is a beautiful day.\"\n",
    "]\n",
    "\n",
    "labels, probs = predict_snli_batch(model, vocab, test_premises, test_hypotheses, device=device)\n",
    "\n",
    "print(\"批量预测结果：\")\n",
    "print(\"-\" * 60)\n",
    "for i, (p, h, label, prob) in enumerate(zip(test_premises, test_hypotheses, labels, probs)):\n",
    "    print(f\"样本 {i+1}:\")\n",
    "    print(f\"  前提: {p}\")\n",
    "    print(f\"  假设: {h}\")\n",
    "    print(f\"  预测: {label}\")\n",
    "    print(f\"  概率: E={prob[0]:.3f}, C={prob[1]:.3f}, N={prob[2]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qtnnq9sd9",
   "metadata": {},
   "source": [
    "## 更难的测试案例\n",
    "\n",
    "下面测试一些更具挑战性的句子对，包括：\n",
    "- 需要常识推理\n",
    "- 细微语义差别\n",
    "- 否定词干扰\n",
    "- 数量推理\n",
    "- 时态/时间推理\n",
    "- 隐含意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wzp9zrnuay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 难度更高的测试案例\n",
    "hard_cases = [\n",
    "    # 1. 否定词推理\n",
    "    (\"The restaurant is not expensive.\", \n",
    "     \"The restaurant is cheap.\",\n",
    "     \"应该是 neutral（不贵≠便宜，可能是中等价位）\"),\n",
    "    \n",
    "    # 2. 数量推理\n",
    "    (\"Three boys are playing soccer.\", \n",
    "     \"Some children are playing a sport.\",\n",
    "     \"应该是 entailment（三个男孩是一些孩子，足球是运动）\"),\n",
    "    \n",
    "    # 3. 常识推理\n",
    "    (\"The man put on his coat before going outside.\", \n",
    "     \"It might be cold outside.\",\n",
    "     \"应该是 neutral（穿外套可能是冷，也可能是下雨或习惯）\"),\n",
    "    \n",
    "    # 4. 细微语义差别\n",
    "    (\"She finished reading the book.\", \n",
    "     \"She read the entire book.\",\n",
    "     \"应该是 entailment（读完=读了整本书）\"),\n",
    "    \n",
    "    # 5. 时态推理\n",
    "    (\"John used to smoke.\", \n",
    "     \"John smokes now.\",\n",
    "     \"应该是 contradiction（used to 暗示现在不抽了）\"),\n",
    "    \n",
    "    # 6. 隐含意义\n",
    "    (\"The student failed the exam.\", \n",
    "     \"The student did not study hard.\",\n",
    "     \"应该是 neutral（考试失败原因很多，不一定是不努力）\"),\n",
    "    \n",
    "    # 7. 双重否定\n",
    "    (\"It is not impossible to finish the task.\", \n",
    "     \"The task can be completed.\",\n",
    "     \"应该是 entailment（不是不可能=可能完成）\"),\n",
    "    \n",
    "    # 8. 比较级推理\n",
    "    (\"Mary is taller than John.\", \n",
    "     \"John is not the tallest person.\",\n",
    "     \"应该是 entailment（Mary比John高，所以John不是最高的）\"),\n",
    "    \n",
    "    # 9. 因果推理\n",
    "    (\"The glass fell off the table.\", \n",
    "     \"The glass is broken.\",\n",
    "     \"应该是 neutral（玻璃掉下不一定碎，取决于高度、地面等）\"),\n",
    "    \n",
    "    # 10. 词汇替换陷阱\n",
    "    (\"The doctor examined the patient.\", \n",
    "     \"The physician checked the sick person.\",\n",
    "     \"应该是 entailment（doctor=physician, patient=sick person, examined≈checked）\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"难度测试案例\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, (premise, hypothesis, expected) in enumerate(hard_cases, 1):\n",
    "    label, probs = predict_snli(model, vocab, premise, hypothesis, device=device)\n",
    "    print(f\"\\n案例 {i}:\")\n",
    "    print(f\"  前提: {premise}\")\n",
    "    print(f\"  假设: {hypothesis}\")\n",
    "    print(f\"  预测: {label}\")\n",
    "    print(f\"  概率: E={probs[0]:.3f}, C={probs[1]:.3f}, N={probs[2]:.3f}\")\n",
    "    print(f\"  期望: {expected}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}