{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5044dcbc",
   "metadata": {},
   "source": [
    "# 自然语言推断与数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8818f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e8110",
   "metadata": {},
   "source": [
    "## SNLI 数据集文件格式说明\n",
    "\n",
    "SNLI 数据集提供了两种格式的文件：\n",
    "\n",
    "### 1. `.txt` 文件（制表符分隔格式，TSV）\n",
    "- **格式**：每行一个样本，字段用制表符（`\\t`）分隔\n",
    "- **表头**：第一行包含字段名\n",
    "- **字段**：\n",
    "  - `gold_label`: 标签（entailment/contradiction/neutral）\n",
    "  - `sentence1`: 前提句子（premise）\n",
    "  - `sentence2`: 假设句子（hypothesis）\n",
    "  - `sentence1_binary_parse`: 句法树（二进制格式）\n",
    "  - `sentence2_binary_parse`: 句法树（二进制格式）\n",
    "  - `sentence1_parse`: 句法树（Penn Treebank格式）\n",
    "  - `sentence2_parse`: 句法树（Penn Treebank格式）\n",
    "  - `captionID`, `pairID`: 唯一标识符\n",
    "  - `label1-5`: 5个标注者的标签\n",
    "- **优点**：易于用 `split('\\t')` 解析，适合简单脚本处理\n",
    "- **示例**：\n",
    "  ```\n",
    "  gold_label\tsentence1\tsentence2\t...\n",
    "  neutral\tTwo women are embracing...\tThe sisters are hugging...\t...\n",
    "  ```\n",
    "\n",
    "### 2. `.jsonl` 文件（JSON Lines 格式）\n",
    "- **格式**：每行一个 JSON 对象\n",
    "- **字段**：与 `.txt` 文件相同，但以 JSON 格式存储\n",
    "- **优点**：\n",
    "  - 结构化数据，易于解析嵌套字段\n",
    "  - 支持复杂数据结构\n",
    "  - 更适合程序化处理\n",
    "- **示例**：\n",
    "  ```json\n",
    "  {\"gold_label\": \"neutral\", \"sentence1\": \"Two women are embracing...\", \"sentence2\": \"The sisters are hugging...\", ...}\n",
    "  ```\n",
    "\n",
    "### 关系总结\n",
    "- **数据内容完全相同**：两种文件包含相同的数据，只是格式不同\n",
    "- **字段对应关系**：`.txt` 的列对应 `.jsonl` 的 JSON 键\n",
    "- **使用建议**：\n",
    "  - 简单文本处理：使用 `.txt` 文件（当前代码使用）\n",
    "  - 需要结构化数据：使用 `.jsonl` 文件（用 `json.loads()` 解析）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7525ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6871af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"\n",
    "    该函数用于读取SNLI（Stanford Natural Language Inference）数据集，并将其解析为前提（premise）、假设（hypothesis）和标签（label）。\n",
    "    \n",
    "    参数:\n",
    "        data_dir (str): 数据集所在的目录\n",
    "        is_train (bool): 是否读取训练集数据。如果为True，则读取训练集；否则读取测试集。\n",
    "\n",
    "    返回:\n",
    "        premises (list of str): 前提句子的列表\n",
    "        hypotheses (list of str): 假设句子的列表\n",
    "        labels (list of int): 标签的列表，0代表entailment，1代表contradiction，2代表neutral\n",
    "    \"\"\"\n",
    "    def extract_text(s):\n",
    "        # 此辅助函数用于对原始文本进行清洗。它将删除括号，\n",
    "        # 并将连续的多个空格替换为一个空格，最后去除首尾空格。\n",
    "        s = re.sub('\\\\(', '', s)        # 去除左括号\n",
    "        s = re.sub('\\\\)', '', s)        # 去除右括号\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)   # 多个空格替换为一个空格\n",
    "        return s.strip()                # 去除首尾空格\n",
    "\n",
    "    # 定义标签与数字的对应关系:\n",
    "    # entailment-蕴涵(0), contradiction-矛盾(1), neutral-中立(2)\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "\n",
    "    # 根据is_train标志选择加载训练集还是测试集\n",
    "    file_name = os.path.join(\n",
    "        data_dir, \n",
    "        'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt'\n",
    "    )\n",
    "\n",
    "    # 打开并读取数据文件，跳过第一行表头\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    # 依次抽取前提、假设和标签\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3d873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is training his horse for a competition .\n",
      "标签： 2\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is at a diner , ordering an omelette .\n",
      "标签： 1\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is outdoors , on a horse .\n",
      "标签： 0\n"
     ]
    }
   ],
   "source": [
    "train_data = read_snli(data_dir, True)\n",
    "for x,y,z in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('前提：', x)\n",
    "    print('假设：', y)\n",
    "    print('标签：', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b829634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各类别样本数（[entailment, contradiction, neutral]）: [183416, 183187, 182764]\n",
      "各类别样本数（[entailment, contradiction, neutral]）: [3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "test_data = read_snli(data_dir, False)\n",
    "# train_data 和 test_data 都是 read_snli 的返回结果\n",
    "# 它们的格式都是 (premises, hypotheses, labels)，即长度为3的元组：\n",
    "# [0] 是前提句子列表， [1] 是假设句子列表， [2] 是标签列表\n",
    "# 所以 data[2] 就是标签列表\n",
    "for data in [train_data, test_data]:\n",
    "    counts = [data[2].count(i) for i in range(3)]\n",
    "    print('各类别样本数（[entailment, contradiction, neutral]）:', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c303e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： ['前提A1', '前提A2']\n",
      "假设： ['假设A1', '假设A2']\n",
      "标签： [0, 1]\n",
      "\n",
      "前提： ['前提B1']\n",
      "假设： ['假设B1']\n",
      "标签： [2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 最简单例子：假设有 train_data 和 test_data 都是 (前提, 假设, 标签) 构成的元组\n",
    "ex1 = ([\"前提A1\", \"前提A2\"], [\"假设A1\", \"假设A2\"], [0, 1])\n",
    "ex2 = ([\"前提B1\"], [\"假设B1\"], [2])\n",
    "for data in [ex1,ex2]:\n",
    "    print(\"前提：\", data[0])\n",
    "    print(\"假设：\", data[1])\n",
    "    print(\"标签：\", data[2])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621fd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, num_steps, vocab=None) -> None:\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            # 这里创建一个词表（vocab），包含前提句子和假设句子的所有分词。min_freq=5表示只保留至少出现5次的词，'reserved_tokens'参数则额外添加了'<pad>'（填充符号）作为特殊标记。\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n",
    "                    min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises= self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "    \n",
    "    def _pad(self, lines):\n",
    "        # 解释：这个方法将每个句子（line）分词后转为词表（vocab）对应的索引序列，然后通过 truncate_pad 函数截断或填充到固定长度 num_steps，不足的用 <pad> 对应的索引补齐。最终所有句子转为形状一致的张量返回，便于批量输入神经网络。\n",
    "        return torch.tensor([d2l.truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>']\n",
    "        ) for line in lines])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx], self.labels[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6568c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    # num_workers = d2l.get_dataloader_workers()\n",
    "    num_workers = 0\n",
    "    data_dir = d2l.download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d8685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1016424",
   "metadata": {},
   "source": [
    "## 数据加载器返回格式说明\n",
    "\n",
    "`SNLIDataset` 的 `__getitem__` 方法返回3个元素：\n",
    "```python\n",
    "return (self.premises[idx], self.hypotheses[idx], self.labels[idx])\n",
    "```\n",
    "\n",
    "因此，`DataLoader` 批量加载时也会返回3个元素的元组：\n",
    "- **premises**: 前提句子的批次，形状 `[batch_size, num_steps]`\n",
    "- **hypotheses**: 假设句子的批次，形状 `[batch_size, num_steps]`\n",
    "- **labels**: 标签的批次，形状 `[batch_size]`\n",
    "\n",
    "**正确的解包方式**：\n",
    "```python\n",
    "for premises, hypotheses, labels in train_iter:\n",
    "    # premises: [batch_size, num_steps]\n",
    "    # hypotheses: [batch_size, num_steps]\n",
    "    # labels: [batch_size]\n",
    "```\n",
    "\n",
    "**错误的解包方式**（会导致 ValueError）：\n",
    "```python\n",
    "for X, Y in train_iter:  # ❌ 期望2个值，但实际有3个\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a3184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premises shape:   torch.Size([128, 50])\n",
      "hypotheses shape: torch.Size([128, 50])\n",
      "labels shape:     torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# SNLIDataset 返回3个元素：(premises, hypotheses, labels)\n",
    "# DataLoader 批量加载时，返回的也是3个元素的元组：\n",
    "# - 第1个元素：premises 批次 [batch_size, num_steps]\n",
    "# - 第2个元素：hypotheses 批次 [batch_size, num_steps]  \n",
    "# - 第3个元素：labels 批次 [batch_size]\n",
    "for premises, hypotheses, labels in train_iter:\n",
    "    print(f\"premises shape:   {premises.shape}\")    # [batch_size, num_steps]\n",
    "    print(f\"hypotheses shape: {hypotheses.shape}\")  # [batch_size, num_steps]\n",
    "    print(f\"labels shape:     {labels.shape}\")      # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4916fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cad12fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "q4tbwaab3zf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为多头注意力变换形状\"\"\"\n",
    "    # X: (batch_size, num_queries/num_keys, num_hiddens)\n",
    "    # -> (batch_size, num_queries/num_keys, num_heads, num_hiddens/num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    # -> (batch_size, num_heads, num_queries/num_keys, num_hiddens/num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # -> (batch_size * num_heads, num_queries/num_keys, num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv的操作\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # 在最后的轴上，被掩蔽的元素使用一个非常大的负值替换，使softmax输出为0\n",
    "        X = X.reshape(-1, shape[-1])\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_lens[:, None]\n",
    "        X[~mask] = -1e6\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"缩放点积注意力\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8c2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80dc038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BertEncoder\n",
    "    segment表示\"句子片段类型embedding\"。\n",
    "    在BERT中，输入通常是两段文本拼接，例如句子A和句子B。\n",
    "    segment用于区分不同的句子（例如A为0，B为1），以便模型能够知道某个token属于哪一部分。\n",
    "\n",
    "    输入是2，代表segment可以取两种类型（0或1）：0表示第一个句子片段，1表示第二个句子片段。\n",
    "    如果只输入单句任务，全部segment为0；如果是句子对任务，根据分割点设置为0和1。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768) -> None:\n",
    "        super().__init__()\n",
    "        # 修复拼写：embeding -> embedding（与预训练模型保持一致）\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # segment_embedding输入2，代表两种类型（句子1和句子2：0或1）\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                                                      ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 位置编码，shape=[1, max_len, num_hiddens]，可学习\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "    \n",
    "    def forward(self, token, segment, valid_lens):\n",
    "        \"\"\"\n",
    "        token: 词索引序列，[batch, seq_len]\n",
    "        segment: 句子片段类型，[batch, seq_len]，值为0或1\n",
    "        valid_lens: 有效长度\n",
    "        \"\"\"\n",
    "        # token embedding + segment embedding\n",
    "        X = self.token_embedding(token) + self.segment_embedding(segment)\n",
    "        # 加上可学习的位置编码\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08949398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768) -> None:\n",
    "        \"\"\"\n",
    "        Masked Language Model（MLM）模块。\n",
    "\n",
    "        参数说明：\n",
    "        vocab_size: 词表大小，输出类别数（即预测每个位置对应的词汇表token）。\n",
    "        num_hiddens: 隐藏层的维度。\n",
    "        num_inputs: 输入特征的维度，通常等于BERT编码器输出的隐藏单元数，默认768。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 构造一个MLP（多层感知机），输入num_inputs维，经过隐藏层后输出vocab_size维的预测。\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens), # 线性变换到隐藏层\n",
    "            nn.ReLU(),                          # 激活函数\n",
    "            nn.LayerNorm(num_hiddens),          # 层归一化\n",
    "            nn.Linear(num_hiddens, vocab_size)  # 映射到vocab_size，为softmax前的logits\n",
    "        )\n",
    "    \n",
    "    def forward(self, X, pred_positions):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "\n",
    "        参数：\n",
    "        X: 经过BERT encoder后的表示，形状为 [batch_size, seq_len, hidden_dim]，\n",
    "           表示每个token的上下文表示。\n",
    "        pred_positions: 要预测的masked位置索引，形状为 [batch_size, num_pred_positions]，\n",
    "                        每行是一个样本要预测的位置列表。\n",
    "\n",
    "        返回：\n",
    "        mlm_Y_hat: 每个被mask位置的预测结果，\n",
    "                   形状为 [batch_size, num_pred_positions, vocab_size]。\n",
    "        \"\"\"\n",
    "        # 1. 得到每个样本需要预测的token数量\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        # 2. 将pred_positions展平为一维，便于统一索引\n",
    "        pred_positions_flat = pred_positions.reshape(-1)  # 长度为batch_size * num_pred_positions\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        # 3. 构造一个batch索引。例如batch_size=2, num_pred_positions=3时，得到[0,0,0,1,1,1]\n",
    "        batch_idx = torch.arange(0, batch_size).repeat_interleave(num_pred_positions)\n",
    "        # 这样(X[batch_idx, pred_positions_flat])就取出所有需要mask的token的表示\n",
    "\n",
    "        # 4. 按指定位置收集得到被mask位置的上下文表示，形状为 [batch_size * num_pred_positions, hidden_dim]\n",
    "        masked_X = X[batch_idx, pred_positions_flat]\n",
    "        # 5. 恢复成 [batch_size, num_pred_positions, hidden_dim] 的形式\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "\n",
    "        # 6. 通过MLP变换，每个位置最终输出vocab_size维，对应softmax前的logits\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "\n",
    "        # 7. 输出，形状为 [batch_size, num_pred_positions, vocab_size]\n",
    "        return mlm_Y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87aecc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, num_inputs) -> None:\n",
    "        super().__init__()\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.output(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58b604bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,\n",
    "                 nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BertEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是\"<cls>\"标记的索引\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "155fd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir,\n",
    "        'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    bert = BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=4, num_layers=2, dropout=0.2,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6476188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22b0371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b17131d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'SNLIBERTDataset' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-17:\n",
      "Process SpawnPoolWorker-18:\n",
      "Process SpawnPoolWorker-19:\n",
      "Process SpawnPoolWorker-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/haoyiwen/miniforge3/envs/d2l/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size, max_len, num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m128\u001b[39m, d2l\u001b[38;5;241m.\u001b[39mget_dataloader_workers()\n\u001b[1;32m      2\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mdownload_extract(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSNLI\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mSNLIBERTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_snli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_set \u001b[38;5;241m=\u001b[39m SNLIBERTDataset(d2l\u001b[38;5;241m.\u001b[39mread_snli(data_dir, \u001b[38;5;28;01mFalse\u001b[39;00m), max_len, vocab)\n\u001b[1;32m      5\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_set, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                                    num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36mSNLIBERTDataset.__init__\u001b[0;34m(self, dataset, max_len, vocab)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m vocab\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len \u001b[38;5;241m=\u001b[39m max_len\n\u001b[1;32m     11\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_token_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_segments,\n\u001b[0;32m---> 12\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_lens) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_premise_hypothesis_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_token_ids)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m, in \u001b[0;36mSNLIBERTDataset._preprocess\u001b[0;34m(self, all_premise_hypothesis_tokens)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, all_premise_hypothesis_tokens):\n\u001b[1;32m     16\u001b[0m     pool \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mPool(\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# 使用4个进程\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mp_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_premise_hypothesis_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     all_token_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m         token_ids \u001b[38;5;28;01mfor\u001b[39;00m token_ids, segments, valid_len \u001b[38;5;129;01min\u001b[39;00m out]\n\u001b[1;32m     20\u001b[0m     all_segments \u001b[38;5;241m=\u001b[39m [segments \u001b[38;5;28;01mfor\u001b[39;00m token_ids, segments, valid_len \u001b[38;5;129;01min\u001b[39;00m out]\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniforge3/envs/d2l/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                   num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                  num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf12d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segment_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segment_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e912b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.0004, 5\n",
    "trainer = torch.optim.AdamW(net.parameters(), lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
