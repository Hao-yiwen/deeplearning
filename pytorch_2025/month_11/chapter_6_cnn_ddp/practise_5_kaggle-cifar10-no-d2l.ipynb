{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d762b6",
   "metadata": {},
   "source": [
    "# 实战 Kaggle 比赛：图像分类 (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf48fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import shutil  # shutil库用于文件和文件夹的高级操作，例如复制、移动、删除等\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import hashlib\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`sec_minibatch_sgd`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"返回第i个GPU设备，如果不存在则返回CPU\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的GPU设备，如果没有GPU则返回[cpu()]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = datasets.FashionMNIST(\n",
    "        root='./data', train=True, transform=trans, download=True)\n",
    "    mnist_test = datasets.FashionMNIST(\n",
    "        root='./data', train=False, transform=trans, download=True)\n",
    "    return (DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "            DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4))\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if device is None:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    \n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = [0.0] * 2\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric[0] += (net(X).argmax(dim=1) == y).sum().item()\n",
    "            metric[1] += y.numel()\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置matplotlib的轴\"\"\"\n",
    "        axes.set_xlabel(xlabel)\n",
    "        axes.set_ylabel(ylabel)\n",
    "        axes.set_xscale(xscale)\n",
    "        axes.set_yscale(yscale)\n",
    "        axes.set_xlim(xlim)\n",
    "        axes.set_ylim(ylim)\n",
    "        if legend:\n",
    "            axes.legend(legend)\n",
    "        axes.grid()\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "class Residual(nn.Module):\n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"ResNet-18模型\"\"\"\n",
    "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual(num_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # ResNet-18\n",
    "    net = nn.Sequential(\n",
    "        nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.LazyBatchNorm2d(), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    )\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1, 1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)\n",
    "    ))\n",
    "    return net\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"使用svg格式显示绘图\"\"\"\n",
    "    from matplotlib_inline import backend_inline\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        # 将tensor或PIL图像转换为numpy数组\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # 如果是PyTorch tensor，转换为numpy\n",
    "            img = img.detach().cpu().numpy()\n",
    "            # 如果是(C, H, W)格式，转换为(H, W, C)\n",
    "            if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "            # 如果是单通道图像，去掉通道维度\n",
    "            if img.shape[-1] == 1:\n",
    "                img = img.squeeze(-1)\n",
    "        ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "\n",
    "\n",
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "\n",
    "\n",
    "def download(name, cache_dir='../data'):\n",
    "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 DATA_HUB\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    \n",
    "    # 如果文件已存在且哈希值匹配，直接返回\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname\n",
    "    \n",
    "    # 下载文件\n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def download_extract(name, folder=None):\n",
    "    \"\"\"下载并解压zip/tar文件\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    \n",
    "    if ext == '.zip':\n",
    "        with zipfile.ZipFile(fname, 'r') as fp:\n",
    "            fp.extractall(base_dir)\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        import tarfile\n",
    "        with tarfile.open(fname, 'r') as fp:\n",
    "            fp.extractall(base_dir)\n",
    "    \n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "\n",
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"用多GPU进行小批量训练\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # 对于BERT微调所需的\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    \n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.mean().backward()  # 使用mean而不是sum，这样梯度更稳定\n",
    "    trainer.step()\n",
    "    \n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum\n",
    "\n",
    "\n",
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices=None):\n",
    "    \"\"\"用GPU训练模型（在第十三章定义）\"\"\"\n",
    "    if devices is None:\n",
    "        devices = try_all_gpus()\n",
    "    \n",
    "    timer, num_batches = Timer(), len(train_iter)\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                       legend=['train loss', 'train acc', 'test acc'])\n",
    "    \n",
    "    # 只有在有多个GPU时才使用DataParallel\n",
    "    if len(devices) > 1 and str(devices[0]).startswith('cuda'):\n",
    "        net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    else:\n",
    "        net = net.to(devices[0])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            \n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                           (metric[0] / metric[2], metric[1] / metric[2], None))\n",
    "        \n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    \n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[2]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af43d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/kaggle_cifar10_tiny.zip from http://d2l-data.s3-accelerate.amazonaws.com/kaggle_cifar10_tiny.zip...\n"
     ]
    }
   ],
   "source": [
    "DATA_HUB['cifar10_tiny'] = (DATA_URL + 'kaggle_cifar10_tiny.zip', '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')\n",
    "\n",
    "demo = True\n",
    "\n",
    "if demo:\n",
    "    data_dir = download_extract('cifar10_tiny')\n",
    "else:\n",
    "    data_dir = \"../data/cifar-10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d4249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 'b'}\n"
     ]
    }
   ],
   "source": [
    "c =dict({\n",
    "    \"a\":\"b\"\n",
    "})\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b76dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 训练样本 : 1000\n",
      "# 类别 : 10\n",
      "1: frog\n",
      "2: truck\n",
      "3: truck\n",
      "4: deer\n",
      "5: automobile\n",
      "6: automobile\n",
      "7: bird\n",
      "8: horse\n",
      "9: ship\n",
      "10: cat\n"
     ]
    }
   ],
   "source": [
    "def read_csv_labels(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    return dict(((name, label) for name,label in tokens))\n",
    "\n",
    "labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "print('# 训练样本 :', len(labels))\n",
    "print('# 类别 :', len(set(labels.values())))\n",
    "for i, (k, v) in enumerate(labels.items()):\n",
    "    if i < 10:\n",
    "        print(f'{k}: {v}')\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad17a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyfile(filename, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    # shutil.copy(filename, target_dir)  # 只复制内容和权限信息，不包括mtime/atime等元数据\n",
    "    shutil.copy2(filename, target_dir)   # 复制内容、权限和大多数元数据（包括时间戳）\n",
    "\n",
    "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
    "    n = collections.Counter(labels.values()).most_common()[-1][1]\n",
    "    n_valid_per_label = max(1, math.floor(n*valid_ratio))\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
    "        label = labels[train_file.split('.')[0]]\n",
    "        fname = os.path.join(data_dir, 'train', train_file)\n",
    "        copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) +1\n",
    "        else:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train', label))\n",
    "    \n",
    "    return n_valid_per_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83a5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_test(data_dir):\n",
    "    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "        copyfile(\n",
    "            os.path.join(data_dir, 'test', test_file),\n",
    "            os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0096761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_cifar10_data(data_dir, valid_ratio):\n",
    "    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    reorg_test(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd5f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 if demo else 128\n",
    "valid_ratio = 0.1\n",
    "reorg_cifar10_data(data_dir, valid_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af165b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/kaggle_cifar10_tiny\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03310e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    # 首先，我们将图片的较短边缩放到40像素（长边等比例缩放），这样为后面的随机裁剪提供更高分辨率的图像基础。\n",
    "    torchvision.transforms.Resize(40),\n",
    "    # 然后，使用RandomResizedCrop随机裁剪一块32x32的区域。裁剪区域的面积占原图的比例为0.6到1之间，裁剪框的宽高比固定为1（即正方形），最后将该区域缩放（若有必要）到32x32。\n",
    "    torchvision.transforms.RandomResizedCrop(32, scale=(0.6, 1), ratio=(1,1)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "transorm_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e53143",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_valid_ds = [\n",
    "    torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'train_valid_test', fold_name),\n",
    "        transform=transform_train, \n",
    "    ) for fold_name in ['train', 'train_valid']\n",
    "]\n",
    "\n",
    "valid_ds, test_ds = [\n",
    "    torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'train_valid_test', fold_name),\n",
    "        transform=transorm_test,\n",
    "    ) for fold_name in ['valid', 'test']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc849595",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, drop_last = True\n",
    ") for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter= torch.utils.data.DataLoader(\n",
    "    valid_ds, batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(\n",
    "    test_ds, batch_size, shuffle=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9400c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    num_classes = 10\n",
    "    net = resnet18(num_classes, 3)\n",
    "    return net\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83209fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_perios, lr_decay):\n",
    "    trainer = torch.optim.SGD(net.parameters(),lr, momentum=0.9, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_perios, lr_decay)\n",
    "    num_batches, timer = len(train_iter), Timer()\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid acc')\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    \n",
    "    net = nn.DataParallel(net, devices).to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i+1) % (num_batches // 5) ==0 or i == num_batches -1:\n",
    "                animator.add(epoch + (i+1)/num_batches, (metric[0]/metric[2], metric[1]/metric[2], None))\n",
    "        \n",
    "        if valid_iter is not None:\n",
    "            valid_acc = evaluate_accuracy_gpu(net, valid_iter, devices[0])\n",
    "            animator.add(epoch +1, (None, None, valid_acc))\n",
    "        scheduler.step()\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f\", valid acc {valid_acc:.3f}\"\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices, num_epochs, lr, wd = try_all_gpus(), 10, 2e-4, 5e-4\n",
    "lr_perios, lr_decay, net = 4, 0.9, get_net()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_perios, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415496d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, preds = get_net(), []\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_perios, lr_decay)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for X,_ in test_iter:\n",
    "        # 这里把X放到devices[0]（比如第0个GPU）上，是因为模型net在训练时已经通过DataParallel放到了所有的devices上。\n",
    "        # 推理/预测时，DataParallel会默认把输入的数据也切分并分发到各个卡上，但如果只用一张卡预测（比如测试集较小或提交时只需要一张卡），\n",
    "        # 就要保证输入和模型都在同一张卡上，以确保forward时不会报“device mismatch”相关的错。\n",
    "        # 这里X.to(devices[0])的作用，就是把输入数据放到模型所在的主卡上（第0个GPU）。\n",
    "\n",
    "        X = X.to(devices[0])\n",
    "        y_hat = net(X)\n",
    "        preds.extend(y_hat.argmax(dim=1).type(torch.float32).cpu().numpy())\n",
    "\n",
    "sorted_ids = list(range(1, len(test_ds) +1))\n",
    "sorted_ids.sort(key = lambda x: str(x))\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "# classes是一个保存所有类别名称的列表，比如['airplane', 'automobile', ...]，它的索引对应模型输出的类别编号。用train_valid_ds.classes[x]可以把预测的类别编号x转换为类别的实际名称。\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.classes[int(x)])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
