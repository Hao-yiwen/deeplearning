{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c646634",
   "metadata": {},
   "source": [
    "# 多gpu训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7de827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66478e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`sec_minibatch_sgd`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"返回第i个GPU设备，如果不存在则返回CPU\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = datasets.FashionMNIST(\n",
    "        root='./data', train=True, transform=trans, download=True)\n",
    "    mnist_test = datasets.FashionMNIST(\n",
    "        root='./data', train=False, transform=trans, download=True)\n",
    "    return (DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "            DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4))\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if device is None:\n",
    "        # 尝试从net获取设备信息\n",
    "        try:\n",
    "            if hasattr(net, 'parameters'):\n",
    "                device = next(iter(net.parameters())).device\n",
    "            else:\n",
    "                # 如果是lambda函数，使用第一个参数来推断设备\n",
    "                device = torch.device('cpu')\n",
    "        except:\n",
    "            device = torch.device('cpu')\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = [0.0] * 2\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric[0] += (net(X).argmax(dim=1) == y).sum().item()\n",
    "            metric[1] += y.numel()\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置matplotlib的轴\"\"\"\n",
    "        axes.set_xlabel(xlabel)\n",
    "        axes.set_ylabel(ylabel)\n",
    "        axes.set_xscale(xscale)\n",
    "        axes.set_yscale(yscale)\n",
    "        axes.set_xlim(xlim)\n",
    "        axes.set_ylim(ylim)\n",
    "        if legend:\n",
    "            axes.legend(legend)\n",
    "        axes.grid()\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3))*scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5))*scale\n",
    "b2 = torch.zeros(50)\n",
    "W3= torch.randn(size=(800, 128))*scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128,10))*scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2,2), stride=(2,2))\n",
    "    h2_conv = F.conv2d(h1, params[2], params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2,2), stride=(2,2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4])+params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# reduction参数决定返回的损失计算方式:\n",
    "# 'none'表示返回每个样本的损失（不进行任何缩减）,\n",
    "# 'mean'表示返回所有样本损失的平均值,\n",
    "# 'sum'表示返回所有样本损失的总和。\n",
    "# 这里选择'none'，以保留每个样本的单独损失值。\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = get_params(params, try_gpu(0))\n",
    "print('b1 权重:', new_params[1])\n",
    "print('b1 梯度:', new_params[1].grad)\n",
    "print(try_gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    \"\"\"将所有设备上的梯度求和并广播到所有设备\"\"\"\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "    for i in range(1, len(data)):\n",
    "        data[i][:] = data[0].to(data[i].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [torch.ones((1,2), device=try_gpu(i)) * (i+1) for i in range(2)]\n",
    "print('allreduce之前：\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('allreduce之后：\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2341701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(try_gpu(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74926ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：nn.parallel.scatter需要CUDA设备，如果没有GPU会报错\n",
    "# 这里提供一个手动实现的scatter函数作为替代\n",
    "def scatter(data, devices):\n",
    "    \"\"\"将数据分散到多个设备上\"\"\"\n",
    "    if len(devices) == 1:\n",
    "        return [data.to(devices[0])]\n",
    "    # 如果数据在CPU上，需要先移动到第一个GPU\n",
    "    if data.device.type == 'cpu':\n",
    "        data = data.to(devices[0])\n",
    "    # 将数据分割并移动到各个设备\n",
    "    chunk_size = data.shape[0] // len(devices)\n",
    "    chunks = []\n",
    "    for i, device in enumerate(devices):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size if i < len(devices) - 1 else data.shape[0]\n",
    "        chunks.append(data[start:end].to(device))\n",
    "    return chunks\n",
    "\n",
    "# 测试scatter函数（如果没有GPU，使用CPU设备）\n",
    "if torch.cuda.is_available():\n",
    "    data = torch.arange(20).reshape(4,5)\n",
    "    devices = [torch.device('cuda:0'), torch.device('cuda:1')]\n",
    "    split_data = scatter(data, devices)\n",
    "    print('input: ', data)\n",
    "    print('load into', devices)\n",
    "    print('output:', split_data)\n",
    "else:\n",
    "    print('CUDA不可用，使用CPU设备进行演示')\n",
    "    data = torch.arange(20).reshape(4,5)\n",
    "    devices = [torch.device('cpu')]\n",
    "    split_data = scatter(data, devices)\n",
    "    print('input: ', data)\n",
    "    print('load into', devices)\n",
    "    print('output:', split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(X, y, devices):\n",
    "    \"\"\"将批次数据分割到多个设备上\"\"\"\n",
    "    assert X.shape[0] == y.shape[0], \"X和y的批次大小必须相同\"\n",
    "    X_shards = scatter(X, devices)\n",
    "    y_shards = scatter(y, devices)\n",
    "    return X_shards, y_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr, batch_size):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    # 在每个GPU上分别计算损失\n",
    "    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n",
    "          for X_shard, y_shard, device_W in zip(\n",
    "              X_shards, y_shards, device_params)]\n",
    "    for l in ls:  # 反向传播在每个GPU上分别执行\n",
    "        l.backward()\n",
    "    # 将每个GPU的所有梯度相加，并将其广播到所有GPU\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce(\n",
    "                [device_params[c][i].grad for c in range(len(devices))])\n",
    "    # 在每个GPU上分别更新模型参数\n",
    "    for param_list in device_params:\n",
    "        with torch.no_grad():\n",
    "            for param in param_list:\n",
    "                param -= lr * param.grad / batch_size\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "    devices = [try_gpu(i) for i in range(num_gpus)]\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    num_epochs = 10\n",
    "    animator = Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            train_batch(X, y, device_params, devices, lr, batch_size)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "        timer.stop()\n",
    "        # 评估模型精度\n",
    "        test_acc = evaluate_accuracy_gpu(\n",
    "            lambda x: lenet(x, device_params[0]), test_iter, devices[0])\n",
    "        animator.add(epoch + 1, (test_acc,))\n",
    "    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'\n",
    "          f'在{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=2, batch_size=256, lr=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
