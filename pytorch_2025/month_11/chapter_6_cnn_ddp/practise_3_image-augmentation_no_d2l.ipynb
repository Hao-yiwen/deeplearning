{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8033ac4",
   "metadata": {},
   "source": [
    "# 图像增广"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb108794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置代理\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "os.environ['NO_PROXY'] = '127.0.0.1,localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60634a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`sec_minibatch_sgd`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"返回第i个GPU设备，如果不存在则返回CPU\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的GPU设备，如果没有GPU则返回[cpu()]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = datasets.FashionMNIST(\n",
    "        root='./data', train=True, transform=trans, download=True)\n",
    "    mnist_test = datasets.FashionMNIST(\n",
    "        root='./data', train=False, transform=trans, download=True)\n",
    "    return (DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "            DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4))\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if device is None:\n",
    "        # 尝试从net获取设备信息\n",
    "        try:\n",
    "            if hasattr(net, 'parameters'):\n",
    "                device = next(iter(net.parameters())).device\n",
    "            else:\n",
    "                # 如果是lambda函数，使用第一个参数来推断设备\n",
    "                device = torch.device('cpu')\n",
    "        except:\n",
    "            device = torch.device('cpu')\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = [0.0] * 2\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric[0] += (net(X).argmax(dim=1) == y).sum().item()\n",
    "            metric[1] += y.numel()\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置matplotlib的轴\"\"\"\n",
    "        axes.set_xlabel(xlabel)\n",
    "        axes.set_ylabel(ylabel)\n",
    "        axes.set_xscale(xscale)\n",
    "        axes.set_yscale(yscale)\n",
    "        axes.set_xlim(xlim)\n",
    "        axes.set_ylim(ylim)\n",
    "        if legend:\n",
    "            axes.legend(legend)\n",
    "        axes.grid()\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "class Residual(nn.Module):\n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"ResNet-18模型\"\"\"\n",
    "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual(num_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # ResNet-18\n",
    "    net = nn.Sequential(\n",
    "        nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.LazyBatchNorm2d(), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    )\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1, 1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)\n",
    "    ))\n",
    "    return net\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"使用svg格式显示绘图\"\"\"\n",
    "    from matplotlib_inline import backend_inline\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        # 将tensor或PIL图像转换为numpy数组\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # 如果是PyTorch tensor，转换为numpy\n",
    "            img = img.detach().cpu().numpy()\n",
    "            # 如果是(C, H, W)格式，转换为(H, W, C)\n",
    "            if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "            # 如果是单通道图像，去掉通道维度\n",
    "            if img.shape[-1] == 1:\n",
    "                img = img.squeeze(-1)\n",
    "        ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d12448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "set_figsize()\n",
    "img = Image.open('../img/cat1.jpg')  # noqa: F821\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):\n",
    "    Y = [aug(img) for _ in range(num_rows * num_cols)]\n",
    "    show_images(Y, num_rows, num_cols, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b820ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "apply(img, torchvision.transforms.RandomHorizontalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.RandomVerticalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_aug = torchvision.transforms.RandomResizedCrop(\n",
    "    (200, 200), scale=(0.1, 1), ratio=(0.5, 2)\n",
    ")\n",
    "apply(img, shape_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c54e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.ColorJitter(\n",
    "    brightness=0.5, contrast=0, saturation=0, hue=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.ColorJitter(\n",
    "    brightness=0, contrast=0, saturation=0, hue=0.5\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_aug = torchvision.transforms.ColorJitter(\n",
    "    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "apply(img, color_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    color_aug,\n",
    "    shape_aug\n",
    "])\n",
    "apply(img, augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = torchvision.datasets.CIFAR10(train=True, root=\"../data\", download=True)\n",
    "show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ec0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifair10(is_train, augs, batch_size):\n",
    "    datasets = torchvision.datasets.CIFAR10(root=\"../data\", train=is_train, transform=augs, download=True)\n",
    "    dataloader = torch.utils.data.DataLoader(datasets, batch_size=batch_size, shuffle=is_train, num_workers=4)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93815e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    '''用多GPU进行训练'''\n",
    "    if isinstance(X, list):\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    # 这里对l进行sum()，是因为我们使用的损失函数（如nn.CrossEntropyLoss）在未设定reduction='sum'时，输出是一个batch里每个样本的loss（即一个向量）。我们希望对整个小批量的loss求和后再反向传播，使多卡或手动归约时能获得与单卡训练一致的梯度缩放。\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices=None):\n",
    "    if devices is None:\n",
    "        devices = try_all_gpus()\n",
    "    timer, num_batchs = Timer(), len(train_iter)\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        # metric[0]: 累加的loss, metric[1]: 累加的正确预测数, metric[2]: 累加的样本数\n",
    "        metric = Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(net, features, labels, loss=loss, trainer=trainer, devices=devices)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i+1)%(num_batchs // 5) == 0 or i == num_batchs -1:\n",
    "                animator.add(\n",
    "                    epoch + (i+1) / num_batchs,\n",
    "                    (metric[0]/metric[2], metric[1]/metric[2], None)\n",
    "                )\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter, devices[0])\n",
    "        animator.add(epoch+1, (None, None, test_acc))\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[2]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c990d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, devices, net = 256, try_all_gpus(), resnet18(10, 3)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "def train_with_data_aug(train_augs, test_augs, net, lr=0.001):\n",
    "    train_iter = load_cifair10(True, train_augs, batch_size)\n",
    "    test_iter = load_cifair10(False, test_augs, batch_size)\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr)\n",
    "    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb26418",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_data_aug(train_augs, test_augs, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316281a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
