{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bad6e37",
   "metadata": {},
   "source": [
    "# kaggle实战 狗的品种识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import shutil  # shutil库用于文件和文件夹的高级操作，例如复制、移动、删除等\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import hashlib\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Defined in :numref:`sec_minibatch_sgd`\"\"\"\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"返回第i个GPU设备，如果不存在则返回CPU\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的GPU设备，如果没有GPU则返回[cpu()]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = datasets.FashionMNIST(\n",
    "        root='./data', train=True, transform=trans, download=True)\n",
    "    mnist_test = datasets.FashionMNIST(\n",
    "        root='./data', train=False, transform=trans, download=True)\n",
    "    return (DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "            DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4))\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if device is None:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    \n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = [0.0] * 2\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric[0] += (net(X).argmax(dim=1) == y).sum().item()\n",
    "            metric[1] += y.numel()\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: self._set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def _set_axes(self, axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"设置matplotlib的轴\"\"\"\n",
    "        axes.set_xlabel(xlabel)\n",
    "        axes.set_ylabel(ylabel)\n",
    "        axes.set_xscale(xscale)\n",
    "        axes.set_yscale(yscale)\n",
    "        axes.set_xlim(xlim)\n",
    "        axes.set_ylim(ylim)\n",
    "        if legend:\n",
    "            axes.legend(legend)\n",
    "        axes.grid()\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "class Residual(nn.Module):\n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"ResNet-18模型\"\"\"\n",
    "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual(num_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # ResNet-18\n",
    "    net = nn.Sequential(\n",
    "        nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.LazyBatchNorm2d(), \n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    )\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1, 1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)\n",
    "    ))\n",
    "    return net\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"使用svg格式显示绘图\"\"\"\n",
    "    from matplotlib_inline import backend_inline\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        # 将tensor或PIL图像转换为numpy数组\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # 如果是PyTorch tensor，转换为numpy\n",
    "            img = img.detach().cpu().numpy()\n",
    "            # 如果是(C, H, W)格式，转换为(H, W, C)\n",
    "            if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "            # 如果是单通道图像，去掉通道维度\n",
    "            if img.shape[-1] == 1:\n",
    "                img = img.squeeze(-1)\n",
    "        ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "\n",
    "\n",
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "\n",
    "\n",
    "def download(name, cache_dir='../data'):\n",
    "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 DATA_HUB\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    \n",
    "    # 如果文件已存在且哈希值匹配，直接返回\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname\n",
    "    \n",
    "    # 下载文件\n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def download_extract(name, folder=None):\n",
    "    \"\"\"下载并解压zip/tar文件\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    \n",
    "    if ext == '.zip':\n",
    "        with zipfile.ZipFile(fname, 'r') as fp:\n",
    "            fp.extractall(base_dir)\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        import tarfile\n",
    "        with tarfile.open(fname, 'r') as fp:\n",
    "            fp.extractall(base_dir)\n",
    "    \n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "\n",
    "def read_csv_labels(fname):\n",
    "    \"\"\"读取CSV文件中的标签\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # 跳过第一行（列名）\n",
    "        lines = f.readlines()[1:]\n",
    "    # 将每行分割为id和label\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    return dict(((name, label) for name, label in tokens))\n",
    "\n",
    "\n",
    "def copyfile(filename, target_dir):\n",
    "    \"\"\"复制文件到目标目录\"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    shutil.copy(filename, target_dir)\n",
    "\n",
    "\n",
    "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
    "    \"\"\"将训练集分割为训练集和验证集\"\"\"\n",
    "    # 训练集中样本最少的类别中的样本数\n",
    "    n = collections.Counter(labels.values()).most_common()[-1][1]\n",
    "    # 验证集中每个类别的样本数\n",
    "    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
    "    label_count = {}\n",
    "    \n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
    "        label = labels[train_file.split('.')[0]]\n",
    "        fname = os.path.join(data_dir, 'train', train_file)\n",
    "        \n",
    "        # 将样本复制到train_valid_test/train_valid或train_valid_test/train\n",
    "        copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train_valid', label))\n",
    "        \n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test', 'train', label))\n",
    "\n",
    "\n",
    "def reorg_test(data_dir):\n",
    "    \"\"\"在预测期间整理测试集以便于读取\"\"\"\n",
    "    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "        copyfile(os.path.join(data_dir, 'test', test_file),\n",
    "                os.path.join(data_dir, 'train_valid_test', 'test', 'unknown'))\n",
    "\n",
    "\n",
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"用多GPU进行小批量训练\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # 对于BERT微调所需的\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    \n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.mean().backward()  # 使用mean而不是sum，这样梯度更稳定\n",
    "    trainer.step()\n",
    "    \n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum\n",
    "\n",
    "\n",
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices=None):\n",
    "    \"\"\"用GPU训练模型（在第十三章定义）\"\"\"\n",
    "    if devices is None:\n",
    "        devices = try_all_gpus()\n",
    "    \n",
    "    timer, num_batches = Timer(), len(train_iter)\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                       legend=['train loss', 'train acc', 'test acc'])\n",
    "    \n",
    "    # 只有在有多个GPU时才使用DataParallel\n",
    "    if len(devices) > 1 and str(devices[0]).startswith('cuda'):\n",
    "        net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    else:\n",
    "        net = net.to(devices[0])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和，训练准确率之和，样本数\n",
    "        metric = Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            \n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                           (metric[0] / metric[2], metric[1] / metric[2], None))\n",
    "        \n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    \n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[2]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import data_parallel\n",
    "\n",
    "\n",
    "DATA_HUB['dog_tiny'] = (DATA_URL + 'kaggle_dog_tiny.zip', '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')\n",
    "print(DATA_HUB)\n",
    "\n",
    "demo = True\n",
    "\n",
    "if demo:\n",
    "    data_dir = download_extract('dog_tiny')\n",
    "else:\n",
    "    data_dir = os.path.join('..', 'data', 'dog-breed-identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_dog_data(data_dir, valid_ratio):\n",
    "    labels = read_csv_labels(os.path.join(data_dir, 'labels.csv'))\n",
    "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    reorg_test(data_dir)\n",
    "\n",
    "batch_size = 32 if demo else 128\n",
    "valid_ratio = 0.1\n",
    "reorg_dog_data(data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56435dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = torchvision.transforms.Compose([\n",
    "    # 随机裁剪一个区域并缩放到224x224，scale控制裁剪区域的面积比例，ratio控制宽高比\n",
    "    torchvision.transforms.RandomResizedCrop(224, scale=(0.08,1.0), ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    # 这一行通过ColorJitter变换对图片的亮度、对比度、饱和度进行随机调整，从而增强训练数据的多样性，提高模型的泛化能力。\n",
    "    torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3592b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
    "\n",
    "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_test) for folder in ['valid', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5619b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
    "                                         drop_last=True)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
    "                                        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff600d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(devices):\n",
    "    \"\"\"\n",
    "    构建用于fine-tune的神经网络。\n",
    "\n",
    "    该函数首先加载预训练的ResNet34模型，并将其特征层部分（features）作为新模型的主干。\n",
    "    然后，在此基础上新建一个输出层(output_new)，用于狗品种分类。\n",
    "    为了加速训练和防止过拟合，预训练主干的参数被冻结（不进行反向传播）。\n",
    "    所有部件移动到所指定的设备上进行运算。\n",
    "\n",
    "    参数:\n",
    "        devices (list): 一组可用的计算设备，比如['cuda:0', 'cuda:1']。\n",
    "\n",
    "    返回:\n",
    "        nn.Sequential: 构造好的模型，包含固定参数的特征提取部分和可训练的自定义输出层。\n",
    "    \"\"\"\n",
    "    # 新建一个空的容器，便于后续组合主干与输出层\n",
    "    finetune_net = nn.Sequential()\n",
    "\n",
    "    # 加载resnet34的预训练模型，作为特征提取器\n",
    "    finetune_net.features = torchvision.models.resnet34(pretrained=True)\n",
    "    # 打印参数名以及参数信息，帮助了解模型结构\n",
    "    print(finetune_net.named_parameters())\n",
    "\n",
    "    # 定义输出层先将1000维（ImageNet类别数）的输出映射到256维，\n",
    "    # 添加ReLU激活，再映射为120维（狗品种数），实现迁移到新数据集\n",
    "    finetune_net.output_new = nn.Sequential(\n",
    "        nn.Linear(1000, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 120)\n",
    "    )\n",
    "\n",
    "    # 将模型加载到指定设备（如GPU），提升运算速度\n",
    "    finetune_net = finetune_net.to(device=devices[0])\n",
    "\n",
    "    # 冻结特征提取部分的参数，不参与训练\n",
    "    for param in finetune_net.features.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # 返回组合好的网络\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d04184",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')  # 改为sum模式\n",
    "def evaluate_loss(data_iter, net, devices):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for features, labels in data_iter:\n",
    "        features, labels = features.to(devices[0]), labels.to(devices[0])\n",
    "        output = net(features)\n",
    "        l = loss(output, labels)\n",
    "        l_sum += l\n",
    "        n += labels.numel()\n",
    "    return (l_sum/n).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period, lr_decay):\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    # 优化器只优化需要梯度的参数（即requires_grad=True的参数），设置学习率、动量和权重衰减\n",
    "    trainer = torch.optim.SGD(\n",
    "        (param for param in net.parameters() if param.requires_grad), \n",
    "        lr=lr, \n",
    "        momentum=0.9, \n",
    "        weight_decay=wd\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), Timer()\n",
    "    legend = ['train loss']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid loss')\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(2)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            features, labels = features.to(devices[0]), labels.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            out_put = net(features)\n",
    "            l = loss(out_put, labels)  # 移除.sum()，因为loss已经定义为reduction='sum'\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(l, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1], None))\n",
    "        measures = f'train loss {metric[0] / metric[1]:.3f}'\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, devices)\n",
    "            animator.add(epoch + 1, (None, valid_loss.detach().cpu()))\n",
    "        scheduler.step()\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid loss {valid_loss:.3f}'\n",
    "    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices, num_epochs, lr, wd = try_all_gpus(), 10, 1e-4, 1e-4\n",
    "lr_period, lr_decay, net = 2, 0.9, get_net(devices)\n",
    "\n",
    "train(net, train_iter=train_iter, valid_iter=valid_iter, num_epochs=num_epochs, lr=lr, wd=wd, devices=devices, lr_period=lr_period, lr_decay=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_net(devices)\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output = torch.nn.functional.softmax(net(data.to(devices[0])), dim=1)\n",
    "    preds.extend(output.cpu().detach().numpy())\n",
    "ids = sorted(os.listdir(\n",
    "    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
