{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4420298",
   "metadata": {},
   "source": [
    "# GRU - 许巍歌词生成\n",
    "\n",
    "本notebook使用GRU模型训练许巍歌词数据，实现中文歌词生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4aa29",
   "metadata": {},
   "outputs": [],
   "source": "# 预设置代码 - 纯PyTorch实现\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport collections\nimport random\nimport os\nimport glob\nimport time\nimport matplotlib.pyplot as plt\n\n# ============ 1. 许巍歌词数据路径 ============\nlyrics_dir = './许巍_5770'\nprint(f\"使用本地歌词目录: {lyrics_dir}\")\n\n# ============ 2. 读取歌词数据 ============\ndef read_xuwei_lyrics():\n    \"\"\"读取目录下所有txt文件\"\"\"\n    all_lines = []\n    txt_files = glob.glob(os.path.join(lyrics_dir, '*.txt'))\n    print(f\"找到 {len(txt_files)} 个歌词文件\")\n    for txt_file in txt_files:\n        with open(txt_file, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n            all_lines.extend([line.strip() for line in lines if line.strip()])\n    return all_lines\n\n# ============ 3. 词表类 ============\nclass Vocab:\n    def __init__(self, tokens=None, min_freq=0):\n        tokens = tokens or []\n        counter = collections.Counter([t for line in tokens for t in line])\n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n        self.idx_to_token = ['<unk>']\n        self.token_to_idx = {'<unk>': 0}\n        for token, freq in self._token_freqs:\n            if freq >= min_freq and token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, 0)\n        return [self[t] for t in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[i] for i in indices]\n\n# ============ 4. 数据加载 ============\ndef load_corpus():\n    lines = read_xuwei_lyrics()\n    tokens = [list(line) for line in lines]  # 字符级分词\n    vocab = Vocab(tokens)\n    corpus = [vocab[t] for line in tokens for t in line]\n    return corpus, vocab\n\n# ============ 5. 序列数据迭代器 ============\ndef seq_data_iter(corpus, batch_size, num_steps):\n    offset = random.randint(0, num_steps)\n    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n    Xs = torch.tensor(corpus[offset: offset + num_tokens]).reshape(batch_size, -1)\n    Ys = torch.tensor(corpus[offset + 1: offset + num_tokens + 1]).reshape(batch_size, -1)\n    num_batches = Xs.shape[1] // num_steps\n    for i in range(0, num_steps * num_batches, num_steps):\n        X = Xs[:, i: i + num_steps]\n        Y = Ys[:, i: i + num_steps]\n        yield X, Y\n\n# ============ 6. GRU模型 ============\nclass GRUModel(nn.Module):\n    def __init__(self, vocab_size, num_hiddens, num_layers=1):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.num_hiddens = num_hiddens\n        self.num_layers = num_layers\n        self.gru = nn.GRU(vocab_size, num_hiddens, num_layers)\n        self.linear = nn.Linear(num_hiddens, vocab_size)\n    \n    def forward(self, inputs, state):\n        X = F.one_hot(inputs.T.long(), self.vocab_size).float()\n        Y, state = self.gru(X, state)\n        output = self.linear(Y.reshape(-1, Y.shape[-1]))\n        return output, state\n    \n    def begin_state(self, batch_size, device):\n        return torch.zeros(self.num_layers, batch_size, self.num_hiddens, device=device)\n\n# ============ 7. 预测函数 ============\ndef predict(prefix, num_preds, model, vocab, device):\n    model.eval()\n    state = model.begin_state(1, device)\n    outputs = [vocab[prefix[0]]]\n    \n    with torch.no_grad():\n        for y in prefix[1:]:\n            x = torch.tensor([[outputs[-1]]], device=device)\n            _, state = model(x, state)\n            outputs.append(vocab[y])\n        \n        for _ in range(num_preds):\n            x = torch.tensor([[outputs[-1]]], device=device)\n            y, state = model(x, state)\n            outputs.append(int(y.argmax(dim=1)))\n    \n    return ''.join(vocab.to_tokens(outputs))\n\n# ============ 8. 训练函数 ============\ndef train(model, corpus, vocab, num_epochs, lr, batch_size, num_steps, device):\n    model.to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n    \n    history = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        state = None\n        total_loss, total_tokens = 0, 0\n        start_time = time.time()\n        \n        for X, Y in seq_data_iter(corpus, batch_size, num_steps):\n            if state is None:\n                state = model.begin_state(batch_size, device)\n            else:\n                state = state.detach()\n            \n            X, Y = X.to(device), Y.T.reshape(-1).to(device)\n            y_hat, state = model(X, state)\n            loss = loss_fn(y_hat, Y.long())\n            \n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss += loss.item() * Y.numel()\n            total_tokens += Y.numel()\n        \n        ppl = math.exp(total_loss / total_tokens)\n        speed = total_tokens / (time.time() - start_time)\n        history.append(ppl)\n        \n        if (epoch + 1) % 50 == 0:\n            print(f'epoch {epoch+1}, 困惑度 {ppl:.1f}, {speed:.0f} tokens/sec')\n            print(f'  生成: {predict(\"我的\", 30, model, vocab, device)}')\n    \n    # 绘制训练曲线\n    plt.figure(figsize=(8, 4))\n    plt.plot(history)\n    plt.xlabel('Epoch')\n    plt.ylabel('Perplexity')\n    plt.title('Training Progress')\n    plt.grid(True)\n    plt.show()\n    \n    return history\n\n# 加载数据\ncorpus, vocab = load_corpus()\nprint(f'语料库大小: {len(corpus)}, 词表大小: {len(vocab)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a5115",
   "metadata": {},
   "outputs": [],
   "source": "# 超参数设置\nbatch_size = 32\nnum_steps = 25\nnum_hiddens = 256\nnum_layers = 1\nnum_epochs = 500\nlr = 1.0\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f'设备: {device}')\nprint(f'批次大小: {batch_size}, 时间步: {num_steps}')\nprint(f'隐藏单元: {num_hiddens}, 层数: {num_layers}')\nprint(f'训练轮数: {num_epochs}, 学习率: {lr}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af764510",
   "metadata": {},
   "outputs": [],
   "source": "# 创建模型并训练\nmodel = GRUModel(len(vocab), num_hiddens, num_layers)\nprint(f'模型参数量: {sum(p.numel() for p in model.parameters()):,}')\n\n# 开始训练\nhistory = train(model, corpus, vocab, num_epochs, lr, batch_size, num_steps, device)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf74b9d",
   "metadata": {},
   "outputs": [],
   "source": "# 歌词生成测试\nprint(\"=\" * 50)\nprint(\"许巍风格歌词生成:\")\nprint(\"=\" * 50)\n\nprefixes = ['曾经', '我的', '蓝莲花', '生活', '你的', '那一']\nfor prefix in prefixes:\n    generated = predict(prefix, 50, model, vocab, device)\n    print(f\"'{prefix}': {generated}\")\n    print()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}