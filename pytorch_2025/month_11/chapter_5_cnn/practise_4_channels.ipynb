{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46baaaf6",
   "metadata": {},
   "source": [
    "# 多输入和多输出通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb95785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bec860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d_multi_in(X, K):\n",
    "    rs = []\n",
    "    for x, k in zip(X, K):\n",
    "        print(x.shape, k.shape)\n",
    "        r = d2l.corr2d(x, k)\n",
    "        print(r.shape)\n",
    "        rs.append(r)\n",
    "    return sum(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636e2236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([1, 2]), tensor([3, 4]))]\n"
     ]
    }
   ],
   "source": [
    "# zip 会把多个可迭代对象（如列表、元组、张量等）中对应位置的元素打包成一个个元组，并返回由这些元组组成的迭代器。\n",
    "a = torch.tensor([[1,2]])\n",
    "b = torch.tensor([[3,4]])\n",
    "zipped = list(zip(a, b))\n",
    "print(zipped)  # 输出: [(tensor([1, 2]), tensor([3, 4]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8f137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) torch.Size([2, 2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([\n",
    "    [\n",
    "        [0.0, 1.0, 2.0],\n",
    "        [3.0, 4.0, 5.0],\n",
    "        [6.0, 7.0, 8.0]\n",
    "    ],\n",
    "    [\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [4.0, 5.0, 6.0],\n",
    "        [7.0, 8.0, 9.0]\n",
    "    ]\n",
    "])\n",
    "K = torch.tensor([\n",
    "    [\n",
    "        [0.0, 1.0],\n",
    "        [2.0, 3.0]\n",
    "    ],\n",
    "    [\n",
    "        [1.0, 2.0],\n",
    "        [3.0, 4.0]\n",
    "    ]\n",
    "])\n",
    "print(X.shape,K.shape)\n",
    "corr2d_multi_in(X, K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d40281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutil_corr2d_in_out(X, K):\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6c759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n",
      "5 6\n"
     ]
    }
   ],
   "source": [
    "torch.stack([torch.tensor([[1,2]]), torch.tensor([[2,3]])],1)\n",
    "\n",
    "# 这样写是不行的，因为 (3,4) 是一个元组，直接 for x, k in (3, 4) 会导致拆包错误。\n",
    "# 正确做法，应该传入一组可拆包为 (x, k) 的子元组，例如：\n",
    "for x, k in [(3, 4), (5, 6)]:\n",
    "    print(x, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5bd5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2, 2])\n",
      "---\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K, K+1, K+2),0)\n",
    "print(K.shape)\n",
    "print('---')\n",
    "mutil_corr2d_in_out(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095acf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutil_corr2d_in_out(X,K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab71f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_mutil_in_out_1x1(X,K):\n",
    "    c_i, h,w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h*w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = torch.matmul(K,X)\n",
    "    return Y.reshape((c_o, h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b487f48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([1, 1])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3]) torch.Size([1, 1])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3]) torch.Size([1, 1])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3]) torch.Size([1, 1])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3]) torch.Size([1, 1])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3]) torch.Size([1, 1])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.normal(0,1,(3,3,3))\n",
    "K=torch.normal(0,1,(2,3,1,1))\n",
    "\n",
    "Y1 = corr2d_mutil_in_out_1x1(X,K)\n",
    "Y2 = mutil_corr2d_in_out(X,K)\n",
    "Y1,Y2\n",
    "assert torch.abs(Y1-Y2).sum() <1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78358e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两次卷积输出形状: torch.Size([1, 1, 6, 6])\n",
      "单次卷积输出形状: torch.Size([1, 1, 6, 6])\n",
      "结果是否接近: True\n",
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 示例：3x3 卷积 + 3x3 卷积\n",
    "k1 = torch.randn(1, 1, 3, 3)  # 第一个卷积核\n",
    "k2 = torch.randn(1, 1, 3, 3)  # 第二个卷积核\n",
    "X = torch.randn(1, 1, 10, 10) # 输入\n",
    "\n",
    "# 方法1：两次卷积\n",
    "y = F.conv2d(X, k1, padding=0)\n",
    "z1 = F.conv2d(y, k2, padding=0)\n",
    "\n",
    "# 方法2：计算等效卷积核\n",
    "# 等效核 = k2 和 k1 的卷积\n",
    "# 计算等效卷积核时，将k2在高和宽维度flip一下，是因为conv2d本身实现的是互相关操作（cross-correlation），而不是数学上的卷积（convolution），卷积与互相关差了一个核的180度翻转（即先在空间上flip）。所以，这里k2.flip([2, 3])就是将k2在最后两个空间维度（高和宽）都翻转，实现真正的卷积操作，获得等效核。\n",
    "k_eq = F.conv2d(k1, k2.flip([2, 3]), padding=2)  # 需要flip做互相关\n",
    "z2 = F.conv2d(X, k_eq, padding=0)\n",
    "\n",
    "print(\"两次卷积输出形状:\", z1.shape)\n",
    "print(\"单次卷积输出形状:\", z2.shape)\n",
    "print(\"结果是否接近:\", torch.allclose(z1, z2, atol=1e-5))\n",
    "print(k_eq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccfc96ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2, 1]],\n",
       "\n",
       "         [[4, 3]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([\n",
    "    [\n",
    "        [\n",
    "            [1,2]\n",
    "        ],\n",
    "        [\n",
    "            [3,4]\n",
    "        ]\n",
    "    ]\n",
    "]).flip([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9284129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始: tensor([1, 2, 3, 4, 5])\n",
      "翻转: tensor([5, 4, 3, 2, 1])\n",
      "\n",
      "原始:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "上下翻转 flip([0]):\n",
      " tensor([[7, 8, 9],\n",
      "        [4, 5, 6],\n",
      "        [1, 2, 3]])\n",
      "\n",
      "左右翻转 flip([1]):\n",
      " tensor([[3, 2, 1],\n",
      "        [6, 5, 4],\n",
      "        [9, 8, 7]])\n",
      "\n",
      "上下+左右翻转 flip([0,1]):\n",
      " tensor([[9, 8, 7],\n",
      "        [6, 5, 4],\n",
      "        [3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1D 例子\n",
    "a = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"原始:\", a)\n",
    "print(\"翻转:\", a.flip([0]))  # [5, 4, 3, 2, 1]\n",
    "\n",
    "# 2D 例子\n",
    "b = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "print(\"\\n原始:\\n\", b)\n",
    "print(\"\\n上下翻转 flip([0]):\\n\", b.flip([0]))\n",
    "print(\"\\n左右翻转 flip([1]):\\n\", b.flip([1]))\n",
    "print(\"\\n上下+左右翻转 flip([0,1]):\\n\", b.flip([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "debd686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 a 形状: torch.Size([3, 4])\n",
      "view 后形状: torch.Size([2, 6])\n",
      "reshape 后形状: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# view 和 reshape 的区别演示\n",
    "a = torch.rand((3, 4))\n",
    "b_view = a.view(2, 6)       # view 要求原内存连续\n",
    "b_reshape = a.reshape(2, 6) # reshape 更灵活，可以自动处理非连续内存情况\n",
    "\n",
    "print(\"原始 a 形状:\", a.shape)\n",
    "print(\"view 后形状:\", b_view.shape)\n",
    "print(\"reshape 后形状:\", b_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ba8815a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 60])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4, 5, 3)\n",
    "x = x.view(3, -1) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4341e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
