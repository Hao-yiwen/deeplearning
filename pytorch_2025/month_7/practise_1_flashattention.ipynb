{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3b02bd",
   "metadata": {},
   "source": [
    "# flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fe9a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 复杂度分析 ===\n",
      "传统 Self-Attention:\n",
      "  时间复杂度: O(N²d)\n",
      "  空间复杂度: O(N²) - 需要存储完整的注意力矩阵\n",
      "\n",
      "Flash Attention:\n",
      "  时间复杂度: O(N²d) - 相同的 FLOPs\n",
      "  空间复杂度: O(Nd) - 只需要存储输出和统计量\n",
      "\n",
      "优势:\n",
      "1. 内存效率: 避免存储 O(N²) 的注意力矩阵\n",
      "2. IO 效率: 减少 GPU HBM 和 SRAM 之间的数据传输\n",
      "3. 可扩展性: 支持更长的序列长度\n",
      "4. 数值稳定性: 在线 softmax 算法更稳定\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== 内存和性能对比 ===\n",
      "序列长度: 1024\n",
      "批次大小: 2\n",
      "模型维度: 512\n",
      "注意力头数: 8\n",
      "\n",
      "传统注意力矩阵内存需求: 64.00 MB\n",
      "Flash Attention 内存需求: 4.00 MB\n",
      "内存节省比例: 93.8%\n",
      "\n",
      "=== 性能测试 ===\n",
      "传统注意力平均时间: 1.05 ms\n",
      "\n",
      "Flash Attention平均时间: 18.29 ms\n",
      "\n",
      "加速比: 0.06x\n",
      "\n",
      "\n",
      "=== 数值正确性检查 ===\n",
      "最大输出差异: 0.173775\n",
      "相对误差: 1.610615\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "\n",
    "class TraditionalAttention(nn.Module):\n",
    "    \"\"\"传统的 Self-Attention 实现\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 计算 Q, K, V\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力分数矩阵 (关键：这里需要存储完整的 seq_len x seq_len 矩阵)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax (内存密集型操作)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 计算输出\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        return self.w_o(output), attention_weights\n",
    "\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    \"\"\"Flash Attention 实现 (简化版本)\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, block_size=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # 计算 Q, K, V\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Flash Attention 分块计算\n",
    "        output = self._flash_attention_forward(Q, K, V, mask)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        return self.w_o(output), None  # Flash Attention 不返回完整的 attention weights\n",
    "    \n",
    "    def _flash_attention_forward(self, Q, K, V, mask=None):\n",
    "        batch_size, n_heads, seq_len, d_k = Q.shape\n",
    "        block_size = min(self.block_size, seq_len)\n",
    "        \n",
    "        # 初始化输出\n",
    "        O = torch.zeros_like(Q)\n",
    "        \n",
    "        # 分块处理\n",
    "        for i in range(0, seq_len, block_size):\n",
    "            end_i = min(i + block_size, seq_len)\n",
    "            \n",
    "            # 当前块的 Q\n",
    "            Q_i = Q[:, :, i:end_i, :]  # [batch, heads, block_size, d_k]\n",
    "            \n",
    "            # 初始化当前块的统计量\n",
    "            max_score = torch.full((batch_size, n_heads, end_i - i, 1), \n",
    "                                 -float('inf'), device=Q.device)\n",
    "            sum_exp = torch.zeros((batch_size, n_heads, end_i - i, 1), device=Q.device)\n",
    "            O_i = torch.zeros((batch_size, n_heads, end_i - i, d_k), device=Q.device)\n",
    "            \n",
    "            # 对每个 K, V 块进行处理\n",
    "            for j in range(0, seq_len, block_size):\n",
    "                end_j = min(j + block_size, seq_len)\n",
    "                \n",
    "                # 当前块的 K, V\n",
    "                K_j = K[:, :, j:end_j, :]  # [batch, heads, block_size, d_k]\n",
    "                V_j = V[:, :, j:end_j, :]  # [batch, heads, block_size, d_k]\n",
    "                \n",
    "                # 计算当前块的注意力分数\n",
    "                S_ij = torch.matmul(Q_i, K_j.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "                \n",
    "                # 应用掩码\n",
    "                if mask is not None:\n",
    "                    mask_block = mask[:, :, i:end_i, j:end_j]\n",
    "                    S_ij = S_ij.masked_fill(mask_block == 0, -float('inf'))\n",
    "                \n",
    "                # 在线 Softmax 更新 (Flash Attention 的核心)\n",
    "                max_score_new = torch.maximum(max_score, S_ij.max(dim=-1, keepdim=True)[0])\n",
    "                \n",
    "                # 重新缩放之前的累积值\n",
    "                scale_old = torch.exp(max_score - max_score_new)\n",
    "                scale_new = torch.exp(S_ij - max_score_new)\n",
    "                \n",
    "                # 更新分母\n",
    "                sum_exp_new = scale_old * sum_exp + scale_new.sum(dim=-1, keepdim=True)\n",
    "                \n",
    "                # 更新输出\n",
    "                O_i = (scale_old * sum_exp / sum_exp_new) * O_i + \\\n",
    "                      (torch.matmul(scale_new, V_j) / sum_exp_new)\n",
    "                \n",
    "                # 更新统计量\n",
    "                max_score = max_score_new\n",
    "                sum_exp = sum_exp_new\n",
    "            \n",
    "            # 将当前块的结果写入输出\n",
    "            O[:, :, i:end_i, :] = O_i\n",
    "        \n",
    "        return O\n",
    "\n",
    "\n",
    "def memory_usage_comparison():\n",
    "    \"\"\"内存使用对比\"\"\"\n",
    "    \n",
    "    # 测试参数\n",
    "    batch_size = 2\n",
    "    seq_len = 1024  # 序列长度\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 创建测试数据\n",
    "    x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "    \n",
    "    # 传统注意力\n",
    "    traditional_attn = TraditionalAttention(d_model, n_heads).to(device)\n",
    "    \n",
    "    # Flash Attention\n",
    "    flash_attn = FlashAttention(d_model, n_heads, block_size=64).to(device)\n",
    "    \n",
    "    print(\"=== 内存和性能对比 ===\")\n",
    "    print(f\"序列长度: {seq_len}\")\n",
    "    print(f\"批次大小: {batch_size}\")\n",
    "    print(f\"模型维度: {d_model}\")\n",
    "    print(f\"注意力头数: {n_heads}\")\n",
    "    print()\n",
    "    \n",
    "    # 传统注意力的理论内存复杂度\n",
    "    attention_matrix_size = batch_size * n_heads * seq_len * seq_len * 4  # float32\n",
    "    print(f\"传统注意力矩阵内存需求: {attention_matrix_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Flash Attention 的理论内存复杂度\n",
    "    flash_memory_size = batch_size * n_heads * seq_len * d_model // n_heads * 4  # float32\n",
    "    print(f\"Flash Attention 内存需求: {flash_memory_size / (1024**2):.2f} MB\")\n",
    "    print(f\"内存节省比例: {(1 - flash_memory_size / attention_matrix_size) * 100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # 性能测试\n",
    "    print(\"=== 性能测试 ===\")\n",
    "    \n",
    "    # 热身\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            _ = traditional_attn(x)\n",
    "            _ = flash_attn(x)\n",
    "    \n",
    "    # 测试传统注意力\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            output_traditional, attn_weights = traditional_attn(x)\n",
    "    \n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    traditional_time = time.time() - start_time\n",
    "    \n",
    "    # 测试 Flash Attention\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            output_flash, _ = flash_attn(x)\n",
    "    \n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    flash_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"传统注意力平均时间: {traditional_time/10*1000:.2f} ms\")\n",
    "    print()\n",
    "    print(f\"Flash Attention平均时间: {flash_time/10*1000:.2f} ms\")\n",
    "    print()\n",
    "    print(f\"加速比: {traditional_time/flash_time:.2f}x\")\n",
    "    print()\n",
    "    \n",
    "    # 数值正确性检查\n",
    "    print(\"\\n=== 数值正确性检查 ===\")\n",
    "    diff = torch.abs(output_traditional - output_flash).max().item()\n",
    "    print(f\"最大输出差异: {diff:.6f}\")\n",
    "    print(f\"相对误差: {diff / torch.abs(output_traditional).max().item():.6f}\")\n",
    "\n",
    "\n",
    "def complexity_analysis():\n",
    "    \"\"\"复杂度分析\"\"\"\n",
    "    print(\"=== 复杂度分析 ===\")\n",
    "    print(\"传统 Self-Attention:\")\n",
    "    print(\"  时间复杂度: O(N²d)\")\n",
    "    print(\"  空间复杂度: O(N²) - 需要存储完整的注意力矩阵\")\n",
    "    print()\n",
    "    print(\"Flash Attention:\")\n",
    "    print(\"  时间复杂度: O(N²d) - 相同的 FLOPs\")\n",
    "    print(\"  空间复杂度: O(Nd) - 只需要存储输出和统计量\")\n",
    "    print()\n",
    "    print(\"优势:\")\n",
    "    print(\"1. 内存效率: 避免存储 O(N²) 的注意力矩阵\")\n",
    "    print(\"2. IO 效率: 减少 GPU HBM 和 SRAM 之间的数据传输\")\n",
    "    print(\"3. 可扩展性: 支持更长的序列长度\")\n",
    "    print(\"4. 数值稳定性: 在线 softmax 算法更稳定\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    complexity_analysis()\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    memory_usage_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74de26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
